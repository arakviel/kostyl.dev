### [Глава 13. Описание конечных интерфейсов](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces)

Определив все сущности, их ответственность и отношения друг с другом, мы переходим непосредственно к разработке API: нам осталось прописать номенклатуру всех объектов, полей, методов и функций в деталях. В этой главе мы дадим сугубо практические советы, как сделать API удобным и понятным.

Важнейшая задача разработчика API — добиться того, чтобы код, написанный поверх API другими разработчиками, легко читался и поддерживался. Помните, что закон больших чисел работает против вас: если какую-то концепцию или сигнатуру вызова можно понять неправильно, значит, её неизбежно будет понимать неправильно всё большее число партнеров по мере роста популярности API.

**NB**: примеры, приведённые в этой главе, прежде всего иллюстрируют проблемы консистентности и читабельности, возникающие при разработке API. Мы не ставим здесь цели дать рекомендации по разработке REST API (такого рода советы будут даны в соответствующем разделе) или стандартных библиотек языков программирования — важен не конкретный синтаксис, а общая идея.

Важное уточнение номер один:

##### [1\. Правила не должны применяться бездумно](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-1)

Правило — это просто кратко сформулированное обобщение опыта. Они не действуют безусловно и не означают, что можно не думать головой. У каждого правила есть какая-то рациональная причина его существования. Если в вашей ситуации нет причин следовать правилу — значит, следовать ему не нужно.

Это соображение применимо ко всем принципам ниже. Если из-за следования правилам у вас получается неудобный, громоздкий, неочевидный API — это повод пересмотреть правила (или API).

Важно понимать, что вы вольны вводить свои собственные конвенции. Например, в некоторых фреймворках сознательно отказываются от парных методов `set_entity` / `get_entity` в пользу одного метода `entity` с опциональным параметром. Важно только проявить последовательность в её применении — если такая конвенция вводится, то абсолютно все методы API должны иметь подобную полиморфную сигнатуру, или по крайней мере должен существовать принцип именования, отличающий такие комбинированные методы от обычных вызовов.

##### [2\. Явное лучше неявного](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-2)

Из названия любой сущности должно быть очевидно, что она делает, и к каким побочным эффектам может привести её использование.

**Плохо**:

```
// Отменяет заказ
order.canceled = true;
```

Неочевидно, что поле состояния можно перезаписывать, и что это действие отменяет заказ.

**Хорошо**:

```
// Отменяет заказ
order.cancel();
```

**Плохо**:

```
// Возвращает агрегированную
// статистику заказов за всё время
orders.getStats()
```

Даже если операция немодифицирующая, но вычислительно дорогая — следует об этом явно индицировать, особенно если вычислительные ресурсы тарифицируются для пользователя; тем более не стоит подбирать значения по умолчанию так, чтобы вызов операции без параметров максимально расходовал ресурсы.

**Хорошо**:

```
// Вычисляет и возвращает агрегированную
// статистику заказов за указанный период
orders.calculateAggregatedStats({
  begin_date: <начало периода>
  end_date: <конец_периода>
});
```

**Стремитесь к тому, чтобы из сигнатуры функции было абсолютно ясно, что она делает, что принимает на вход и что возвращает**. Вообще, при прочтении кода, работающего с вашим API, должно быть сразу понятно, что, собственно, он делает — без подглядывания в документацию.

Два важных следствия:

**1.1.** Если операция модифицирующая, это должно быть очевидно из сигнатуры. В частности, модифицирующая операция не может называться `getSomething` или использоваться с HTTP-глаголом `GET`.

**1.2.** Если в номенклатуре вашего API есть как синхронные операции, так и асинхронные, то (а)синхронность должна быть очевидна из сигнатур, **либо** должна существовать конвенция именования, позволяющая отличать синхронные операции от асинхронных.

##### [3\. Указывайте использованные стандарты](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-3)

К сожалению, человечество не в состоянии договориться о таких простейших вещах, как «с какого дня начинается неделя». Поэтому _всегда_ указывайте, по какому конкретно стандарту вы отдаёте те или иные величины. Исключения возможны только там, где вы на 100% уверены, что в мире существует только один стандарт для этой сущности, и всё население земного шара о нём в курсе.

**Плохо**: `"date": "11/12/2020"` — существует огромное количество стандартов записи дат, плюс из этой записи невозможно даже понять, что здесь число, а что месяц.

**Хорошо**: `"iso_date": "2020-11-12"`.

**Плохо**: `"duration": 5000` — пять тысяч чего?

**Хорошо**:

`"duration_ms": 5000`

либо

`"duration": "5000ms"` либо `"iso_duration": "PT5S"` либо

```
"duration": {
  "unit": "ms",
  "value": 5000
}
```

Отдельное следствие из этого правила — денежные величины _всегда_ должны сопровождаться указанием кода валюты.

Также следует отметить, что в некоторых областях ситуация со стандартами настолько плоха, что, как ни сделай, — кто-то останется недовольным. Классический пример такого рода — порядок географических координат («широта-долгота» против «долгота-широта»). Здесь, увы, есть только один работающий метод борьбы с фрустрацией — Блокнот душевного покоя, который будет описан [в одноимённой главе](https://twirl.github.io/The-API-Book/API.ru.html#back-compat-serenity-notepad).

##### [4\. Сущности должны именоваться конкретно](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-4)

Избегайте одиночных слов-«амёб» без определённой семантики, таких как get, apply, make.

**Плохо**: `user.get()` — неочевидно, что конкретно будет возвращено.

**Хорошо**: `user.get_id()`.

##### [5\. Не экономьте буквы](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-5)

В XXI веке давно уже нет нужды называть переменные покороче.

**Плохо**: `order.getTime()` — неясно, о каком времени идёт речь: время создания заказа, время готовности заказа, время ожидания заказа?…

**Хорошо**: `order.getEstimatedDeliveryTime()`.

**Плохо**:

```
// возвращает положение
// первого вхождения в строку str1
// любого символа из строки str2
strpbrk (str1, str2)
```

Возможно, автору этого API казалось, что аббревиатура `pbrk` что-то значит для читателя, но он явно ошибся. К тому же, невозможно сходу понять, какая из строк `str1`, `str2` является набором символов для поиска.

**Хорошо**:

```
str_search_for_characters(
  str,
  lookup_character_set
)
```

— однако необходимость существования такого метода вообще вызывает сомнения, достаточно было бы иметь удобную функцию поиска подстроки с нужными параметрами. Аналогично сокращение `string` до `str` выглядит совершенно бессмысленным, но, увы, является устоявшимся для большого количества предметных областей.

**NB**: иногда названия полей сокращают или вовсе опускают (например, возвращают массив разнородных объектов вместо набора именованных полей) в погоне за уменьшением количества трафика. В абсолютном большинстве случаев это бессмысленно, поскольку текстовые данные при передаче обычно дополнительно сжимают на уровне протокола.

##### [6\. Тип поля должен быть ясен из его названия](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-6)

Если поле называется `recipe` — мы ожидаем, что его значением является сущность типа `Recipe`. Если поле называется `recipe_id` — мы ожидаем, что его значением является идентификатор, который мы сможем найти в составе сущности `Recipe`.

То же касается и базовых типов. Сущности-массивы должны именоваться во множественном числе или собирательными выражениями — `objects`, `children`; если это невозможно (термин неисчисляем), следует добавить префикс или постфикс, не оставляющий сомнений.

**Плохо**: `GET /news` — неясно, будет ли получена какая-то конкретная новость или массив новостей.

**Хорошо**: `GET /news-list`.

Аналогично, если ожидается булево значение, то это должно быть очевидно из названия, т.е. именование должно описывать некоторое качественное состояние, например, `is_ready`, `open_now`.

**Плохо**: `"task.status": true` — неочевидно, что статус бинарен, к тому же такой API будет нерасширяемым.

**Хорошо**: `"task.is_finished": true`.

Отдельно следует оговорить, что на разных платформах эти правила следует дополнить по-своему с учётом специфики first-class citizen-типов. Например, в JSON не существует объектов типа `Date`, и даты приходится передавать в виде числа или строки; разумно такие даты индицировать с помощью, например, постфикса `_at` (`created_at`, `occurred_at` и т.д.) или `_date`.

Если наименование сущности само по себе является каким-либо термином, способным смутить разработчика, лучше добавить лишний префикс или постфикс во избежание непонимания.

**Плохо**:

```
// Возвращает список
// встроенных функций кофемашины
GET /coffee-machines/{id}/functions
```

Слово "functions" многозначное: оно может означать и встроенные функции, и написанный код, и состояние (функционирует или не функционирует).

**Хорошо**:

```
GET /v1/coffee-machines/{id}↵
  /builtin-functions-list
```

##### [7\. Подобные сущности должны называться подобно и вести себя подобным образом](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-7)

**Плохо**: `begin_transition` / `stop_transition`

— `begin` и `stop` — непарные термины; разработчик будет вынужден рыться в документации.

**Хорошо**: `begin_transition` / `end_transition` либо `start_transition` / `stop_transition`.

**Плохо**:

```
// Находит первую позицию строки `needle`
// внутри строки `haystack`
strpos(haystack, needle)
// Находит и заменяет
// все вхождения строки `needle`
// внутри строки `haystack`
// на строку `replace`
str_replace(needle, replace, haystack)
```

Здесь нарушены сразу несколько правил:

- написание неконсистентно в части знака подчёркивания;
- близкие по смыслу методы имеют разный порядок аргументов `needle`/`haystack`;
- первый из методов находит только первое вхождение строки `needle`, а другой — все вхождения, и об этом поведении никак нельзя узнать из сигнатуры функций.

Упражнение «как сделать эти интерфейсы хорошо» предоставим читателю.

##### [8\. Избегайте двойных отрицаний](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-8)

**Плохо**: `"dont_call_me": false`

— люди в целом плохо считывают двойные отрицания. Это провоцирует ошибки.

**Лучше**: `"prohibit_calling": true` или `"avoid_calling": true`

— читается лучше, хотя обольщаться всё равно не следует. Насколько это возможно откажитесь от семантически двойных отрицаний, даже если вы придумали «негативное» слово без явной приставки «не».

Стоит также отметить, что в использовании законов де Моргана<sup><a id="api-design-describing-interfaces-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-1">1</a></sup> ошибиться ещё проще, чем в двойных отрицаниях. Предположим, что у вас есть два флага:

```
GET /coffee-machines/{id}/stocks
→
{
  "has_beans": true,
  "has_cup": true
}
```

Условие «кофе можно приготовить» будет выглядеть как `has_beans && has_cup` — есть и зерно, и стакан. Однако, если по какой-то причине в ответе будут отрицания тех же флагов:

```
{
  "no_beans": false,
  "no_cup": false
}
```

— то разработчику потребуется вычислить флаг `!no_beans && !no_cup`, что эквивалентно `!(no_beans || no_cup)`, а вот в этом переходе ошибиться очень легко, и избегание двойных отрицаний помогает слабо. Здесь, к сожалению, есть только общий совет «избегайте ситуаций, когда разработчику нужно вычислять такие флаги».

##### [9\. Избегайте неявного приведения типов](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-9)

Этот совет парадоксально противоположен предыдущему. Часто при разработке API возникает ситуация, когда добавляется новое необязательное поле с непустым значением по умолчанию. Например:

```
let orderParams = {
  contactless_delivery: false
};
let order = api.createOrder(
  orderParams
);
```

Новая опция `contactless_delivery` является необязательной, однако её значение по умолчанию — `true`. Возникает вопрос, каким образом разработчик должен отличить явное _нежелание_ пользоваться опцией (`false`) от незнания о её существовании (поле не задано). Приходится писать что-то типа такого:

```
let value = orderParams.contactless_delivery;
if (Type(value) == 'Boolean' && value == false) {
  …
}
```

Эта практика ведёт к усложнению кода, который пишут разработчики, и в этом коде легко допустить ошибку, которая по сути меняет значение поля на противоположное. То же самое произойдёт, если для индикации отсутствия значения поля использовать специальное значение типа `null` или `-1`.

Если протоколом не предусмотрена нативная поддержка таких кейсов (т.е. разработчик не может допустить ошибку, спутав отсутствие поля с пустым значением), универсальное правило — все новые необязательные булевы флаги должны иметь значение по умолчанию false.

**Хорошо**

```
let orderParams = {
  force_contact_delivery: true
};
let order = api.createOrder(
  orderParams
);
```

Если же требуется ввести небулево поле, отсутствие которого трактуется специальным образом, то следует ввести пару полей.

**Плохо**:

```
// Создаёт пользователя
POST /v1/users
{ … }
→
// Пользователи создаются по умолчанию
// с указанием лимита трат в месяц
{
  "spending_monthly_limit_usd": "100",
  …
}
// Для отмены лимита требуется
// указать значение null
PUT /v1/users/{id}
{
  "spending_monthly_limit_usd": null,
  …
}
```

**Хорошо**

```
POST /v1/users
{
  // true — у пользователя снят
  //   лимит трат в месяц
  // false — лимит не снят
  //   (значение по умолчанию)
  "abolish_spending_limit": false,
  // Необязательное поле, имеет смысл
  // только если предыдущий флаг
  // имеет значение false
  "spending_monthly_limit_usd": "100",
  …
}
```

**NB**: противоречие с предыдущим советом состоит в том, что мы специально ввели отрицающий флаг («нет лимита»), который по правилу двойных отрицаний пришлось переименовать в `abolish_spending_limit`. Хотя это и хорошее название для отрицательного флага, семантика его довольно неочевидна, разработчикам придётся как минимум покопаться в документации. Таков путь.

##### [10\. Декларируйте технические ограничения явно](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-10)

У любого поля в вашем API есть ограничения на допустимые значения: максимальная длина текста, объём прикладываемых документов в мегабайтах, разрешённые диапазоны цифровых значений. Часто разработчики API пренебрегают указанием этих лимитов — либо потому, что считают их очевидными, либо потому, что попросту не знают их сами. Это, разумеется, один большой антипаттерн: незнание пределов использования системы автоматически означает, что код партнёров может в любой момент перестать работать по не зависящим от них причинам.

Поэтому, во-первых, указывайте границы допустимых значений для всех без исключения полей в API, и, во-вторых, если эти границы нарушены, генерируйте машиночитаемую ошибку с описанием, какое ограничение на какое поле было нарушено.

То же соображение применимо и к квотам: партнёры должны иметь доступ к информации о том, какую долю доступных ресурсов они выбрали, и ошибки в случае превышения квоты должны быть информативными.

##### [11\. Любые запросы должны быть лимитированы](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-11)

Ограничения должны быть не только на размеры полей, но и на размеры списков или агрегируемых интервалов.

**Плохо**: `getOrders()` — что, если пользователь совершил миллион заказов?

**Хорошо**: `getOrders({ limit, parameters })` — должно существовать ограничение сверху на размер обрабатываемых и возвращаемых данных и, соответственно, возможность уточнить запрос, если партнёру всё-таки требуется большее количество данных, чем разрешено обрабатывать в одном запросе.

##### [12\. Описывайте политику перезапросов](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-12)

Одна из самых больших проблем с точки зрения производительности, с которой сталкивается почти любой разработчик API, и внутренних, и публичных — это отказ в обслуживании вследствие лавины перезапросов: временные проблемы на бэкенде API (например, повышение времени ответа) могут привести к полной неработоспособности сервера, если клиенты начнут очень быстро повторять запрос, не получив или не дождавшись ответа, сгенерировав, таким образом, кратно большую нагрузку в короткий срок.

Лучшая практика в такой ситуации — это требовать, чтобы клиенты перезапрашивали эндпойнты API с увеличивающимся интервалом (скажем, первый перезапрос происходит через одну секунду, второй — через две, третий через четыре, и так далее, но не больше одной минуты). Конечно, в случае публичного API такое требование никто не обязан соблюдать, но и хуже от его наличия вам точно не станет: хотя бы часть партнёров прочитает документацию и последует вашим рекомендациям.

Кроме того, вы можете разработать референсную реализацию политики перезапросов в ваших публичных SDK и следить за правильностью имплементации open-source модулей к вашему API.

##### [13\. Считайте трафик](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-13)

В современном мире такой ресурс, как объём переданного трафика, считать уже почти не принято — считается, что Интернет всюду практически безлимитен. Однако он всё-таки не абсолютно безлимитен: всегда можно спроектировать систему так, что объём трафика окажется некомфортным даже и для современных сетей.

Три основные причины раздувания объёма трафика достаточно очевидны:

- клиент слишком часто запрашивает данные и/или слишком мало их кэширует;
- не предусмотрен постраничный перебор данных;
- не предусмотрены ограничения на размер значений полей и/или передаются большие бинарные данные (графика, аудио, видео и т.д.).

Все эти проблемы должны решаться через введения ограничений на размеры полей и правильную декомпозицию эндпойнтов. Если в рамках одной сущности необходимо предоставлять как «лёгкие» (скажем, название и описание рецепта), так и «тяжёлые» данные (скажем, промо-фотография напитка, которая легко может по размеру превышать текстовые поля в сотни раз), лучше разделить эндпойнты и отдавать только ссылку для доступа к «тяжёлым» данным (в нашем случае, ссылку на изображение) — это, как минимум, позволит задавать различные политики кэширования для разных данных.

Неплохим упражнением здесь будет промоделировать типовой жизненный цикл основной функциональности приложения партнёра (например, выполнение одного заказа) и подсчитать общее количество запросов и объём трафика на один цикл. Причиной слишком большого числа запросов / объёма трафика может оказаться ошибка проектирования подсистемы уведомлений об изменениях состояния. Подробнее этот вопрос мы рассмотрим в главе [«Двунаправленные потоки данных»](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-push-vs-poll) раздела «Паттерны API».

##### [14\. Отсутствие результата — тоже результат](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-14)

Если сервер корректно обработал вопрос и никакой внештатной ситуации не возникло — следовательно, это не ошибка. К сожалению, весьма распространён антипаттерн, когда отсутствие результата считается ошибкой.

**Плохо**

```
POST /v1/coffee-machines/search
{
  "query": "lungo",
  "location": <положение пользователя>
}
→ 404 Not Found
{
  "localized_message":
    "Рядом с вами не делают лунго"
}
```

Статусы `4xx` означают, что клиент допустил ошибку; однако в данном случае никакой ошибки сделано не было ни пользователем, ни разработчиком: клиент же не может знать заранее, готовят здесь лунго или нет.

**Хорошо**:

```
POST /v1/coffee-machines/search
{
  "query": "lungo",
  "location": <положение пользователя>
}
→ 200 OK
{
  "results": []
}
```

Это правило вообще можно упростить до следующего: если результатом операции является массив данных, то пустота этого массива — не ошибка, а штатный ответ. (Если, конечно, он допустим по смыслу; пустой массив координат, например, является ошибкой.)

**NB**: этот паттерн следует применять и в обратную сторону. Если в запросе можно указать массив сущностей, то следует отличать пустой массив от отсутствия параметра. Рассмотрим следующий пример:

```
// Находит все рецепты кофе
// без молока
POST /v1/recipes/search
{
  "filter": {
    "no_milk": true
  }
}
→ 200 OK
{
  "results": [{
    "recipe": "espresso"
    …
  }, {
    "recipe": "lungo",
    …
  }]
}
```

```
// Находит все предложения
// указанных рецептов
POST /v1/offers/search
{
  "location",
  "recipes": [
    "espresso",
    "lungo"
  ]
}
```

Представим теперь, что вызов первого метода вернул пустой массив результатов, т.е. ни одного рецепта кофе, удовлетворяющего условиям, не было найдено. Хорошо, если разработчик партнёра предусмотрит эту ситуацию и не будет делать запрос поиска предложений — но мы не можем быть стопроцентно в этом уверены. Если обработка пустого массива рецептов не предусмотрена, то приложение партнёра выполнит вот такой запрос:

```
POST /v1/offers/search
{
  "location",
  "recipes": []
}
```

Часто можно столкнуться с ситуацией, когда эндпойнт просто проигнорирует наличие пустого массива `recipes` и вернёт предложения так, как будто никакого фильтра по рецепту передано не было. В нашем примере это будет означать, что приложение просто проигнорирует требование пользователя показать только напитки без молока, что мы никак не можем счесть приемлемым поведением. Поэтому ответом на такой запрос с пустым массивом в качестве параметра должна быть либо ошибка, либо пустой же массив предложений.

##### [15\. Валидируйте ввод](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-15)

Какой из вариантов действий выбрать в предыдущем примере — исключение или пустой ответ — напрямую зависит от того, что записано в вашем контракте. Если спецификация прямо предписывает, что массив `recipes` должен быть непустым, то необходимо сгенерировать исключение (иначе вы фактически нарушаете собственную спецификацию).

Это верно не только в случае непустых массивов, но и любых других зафиксированных в контракте ограничений. «Тихое» исправление недопустимых значений почти никогда не имеет никакого практического смысла:

**Плохо**:

```
POST /v1/offers/search
{
  "location": {
    "longitude": 20,
    "latitude": 100
  }
}
→ 200 OK
{
  // Предложения для точки
  // [0, 90]
  "offers"
}
```

Мы видим, что разработчик по какой-то причине передал некорректное значение широты (100 градусов). Да, мы можем его «исправить», т.е. редуцировать до ближайшего допустимого значения (90 градусов), но кому от этого стало лучше? Разработчик никогда не узнает о допущенной ошибке, а конечному пользователю предложения кофе на Северном полюсе, скорее всего, нерелевантны.

**Хорошо**:

```
POST /v1/coffee-machines/search
{
  "location": {
    "longitude": 20,
    "latitude": 100
  }
}
→ 400 Bad Request
{
  // описание ошибки
}
```

Желательно не только обращать внимание партнёров на ошибки, но и проактивно предупреждать их о поведении, возможно похожем на ошибку:

```
POST /v1/coffee-machines/search
{
  "location": {
    "latitude": 0,
    "longitude": 0
  }
}
→
{
  "results": [],
  "warnings": [{
    "type": "suspicious_coordinates",
    "message": "Location [0, 0]↵
      is probably a mistake"
  }, {
    "type": "unknown_field",
    "message": "unknown field:↵
      `force_convact_delivery`. Did you↵
      mean `force_contact_delivery`?"
  }]
}
```

Однако следует отметить, что далеко не во все интерфейсы можно удобно уложить дополнительно возврат предупреждений. В такой ситуации можно ввести дополнительный режим отладки или строгий режим, в котором уровень предупреждений эскалируется:

```
POST /v1/coffee-machines/search↵
  ?strict_mode=true
{
  "location": {
    "latitude": 0,
    "longitude": 0
  }
}
→ 400 Bad Request
{
  "errors": [{
    "type": "suspicious_coordinates",
    "message": "Location [0, 0]↵
      is probably a mistake"
  }],
  …
}
```

Если всё-таки координаты \[0, 0\] не ошибка, то можно дополнительно разрешить задавать игнорируемые ошибки для конкретной операции:

```
POST /v1/coffee-machines/search↵
  ?strict_mode=true↵
  &disable_errors=suspicious_coordinates
```

##### [16\. Значения по умолчанию должны быть осмысленны](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-16)

Значения по умолчанию — один из самых ваших сильных инструментов, позволяющих избежать многословности при работе с API. Однако эти умолчания должны помогать разработчикам, а не маскировать их ошибки.

**Плохо**:

```
POST /v1/coffee-machines/search
{
  "recipes": ["lungo"]
  // Положение пользователя не задано
}
→
{
  "results": [
    // Результаты для какой-то
    // локации по умолчанию
  ]
}
```

Формально, подобное умолчание допустимо — почему бы не иметь концепции «географических координат по умолчанию». Однако в реальности результатом подобных политик «тихого» исправления ошибок становятся абсурдные ситуации типа «null island» — самой посещаемой точки в мире<sup><a id="api-design-describing-interfaces-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-2">2</a></sup>. Чем популярнее API, тем больше шансов, что партнеры просто не обратят внимания на такие пограничные ситуации.

**Хорошо**:

```
POST /v1/coffee-machines/search
{
  "recipes": ["lungo"]
  // Положение пользователя не задано
}
→ 400 Bad Request
{
  // описание ошибки
}
```

##### [17\. Ошибки должны быть информативными](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-17)

Недостаточно просто валидировать ввод — необходимо ещё и уметь правильно описать, в чём состоит проблема. В ходе работы над интеграцией партнёры неизбежно будут допускать детские ошибки. Чем понятнее тексты сообщений, возвращаемых вашим API, тем меньше времени разработчик потратит на отладку, и тем приятнее работать с таким API.

**Плохо**:

```
POST /v1/coffee-machines/search
{
  "recipes": ["lngo"],
  "position": {
    "latitude": 110,
    "longitude": 55
  }
}
→ 400 Bad Request
{}
```

— да, конечно, допущенные ошибки (опечатка в `"lngo"` и неправильные координаты) очевидны. Но раз наш сервер всё равно их проверяет, почему не вернуть описание ошибок в читаемом виде?

**Хорошо**:

```
{
  "reason": "wrong_parameter_value",
  "localized_message":
    "Что-то пошло не так.↵
     Обратитесь к разработчику приложения.",
  "details": {
    "checks_failed": [
      {
        "field": "recipe",
        "error_type": "wrong_value",
        "message":
          "Value 'lngo' unknown.↵
           Did you mean 'lungo'?"
      },
      {
        "field": "position.latitude",
        "error_type": "constraint_violation",
        "constraints": {
          "min": -90,
          "max": 90
        },
        "message":
          "'position.latitude' value↵
          must fall within↵
          the [-90, 90] interval"
      }
    ]
  }
}
```

Также хорошей практикой является указание всех допущенных ошибок, а не только первой найденной.

##### [18\. Всегда показывайте неразрешимые ошибки прежде разрешимых](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-18)

Рассмотрим пример с заказом кофе

```
POST /v1/orders
{
  // запрошенный рецепт
  "recipe": "lngo",
  // идентификатор предложения
  "offer"
}
→ 409 Conflict
{
  // ошибка: время действия
  // предложения истекло
  "reason": "offer_expired"
}
```

```
// Повторный запрос
// с новым `offer`
POST /v1/orders
{
  "recipe": "lngo",
  "offer"
}
→ 400 Bad Request
{
  // Ошибка: неизвестный рецепт
  "reason": "recipe_unknown"
}
```

Какой был смысл получать новый `offer`, если заказ всё равно не может быть создан? Для пользователя это будет выглядеть как бессмысленные действия (или бессмысленное ожидание), которые всё равно завершатся ошибкой, что бы он ни делал. Да, соблюдение порядка ошибок не изменит результат — заказ всё ещё нельзя сделать — но, во-первых, пользователь потратит меньше времени (а также сделает меньше запросов к бэкенду и внесёт меньший вклад в фон ошибок) и, во-вторых, диагностика проблемы будет гораздо проще читаться.

##### [19\. Начинайте исправление ошибок с более глобальных](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-19)

Если ошибки исправимы (т.е. пользователь может совершить какие-то действия и всё же добиться желаемого), следует в первую очередь сообщать о тех, которые потребуют более глобального изменения состояния.

**Плохо**:

```
POST /v1/orders
{
  "items": [{
    "item_id": "123",
    "price": "0.10"
  }]
}
→
409 Conflict
{
  // Ошибка: пока пользователь
  // совершал заказ, цена
  // товара изменилась
  "reason": "price_changed",
  "details": [{
    "item_id": "123",
    "actual_price": "0.20"
  }]
}
```

```
// Повторный запрос
// с актуальной ценой
POST /v1/orders
{
  "items": [{
    "item_id": "123",
    "price": "0.20"
  }]
}
→
409 Conflict
{
  // Ошибка: у пользователя слишком
  // много одновременных заказов,
  // создание новых заказов запрещено
  "reason": "order_limit_exceeded",
  "localized_message":
    "Лимит заказов превышен"
}
```

Какой был смысл показывать пользователю диалог об изменившейся цене, если и с правильной ценой заказ он сделать всё равно не сможет? Пока один из его предыдущих заказов завершится и можно будет сделать следующий заказ, цену, наличие и другие параметры заказа всё равно придётся корректировать ещё раз.

##### [20\. Проанализируйте потенциальные взаимные блокировки](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-20)

В сложных системах не редки ситуации, когда исправление одной ошибки приводит к возникновению другой и наоборот.

```
// Создаём заказ с платной доставкой
POST /v1/orders
{
  "items": 3,
  "item_price": "3000.00"
  "currency_code": "MNT",
  "delivery_fee": "1000.00",
  "total": "10000.00"
}
→ 409 Conflict
// Ошибка: доставка становится бесплатной
// при стоимости заказа от 9000 тугриков
{
  "reason": "delivery_is_free"
}
```

```
// Создаём заказ с бесплатной доставкой
POST /v1/orders
{
  "items": 3,
  "item_price": "3000.00"
  "currency_code": "MNT",
  "delivery_fee": "0.00",
  "total": "9000.00"
}
→ 409 Conflict
// Ошибка: минимальная сумма заказа
// 10000 тугриков
{
  "reason": "below_minimal_sum",
  "currency_code": "MNT",
  "minimal_sum": "10000.00"
}
```

Легко заметить, что в этом примере нет способа разрешить ошибку в один шаг — эту ситуацию требуется предусмотреть отдельно, и либо изменить параметры расчёта (минимальная сумма заказа не учитывает скидки), либо ввести специальную ошибку для такого кейса.

##### [21\. Указывайте время жизни ресурсов и политики кэширования](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-21)

В современных системах клиент, как правило, обладает собственным состоянием и почти всегда кэширует результаты запросов — неважно, долговременно ли или в течение сессии: у каждого объекта всегда есть какое-то время автономной жизни. Поэтому желательно вносить ясность; каким образом рекомендуется кэшировать результат должно быть понятно, если не из сигнатур операций, то хотя бы из документации.

Следует уточнить, что кэш мы понимаем в расширенном смысле, а именно: какое варьирование параметров операции (не только времени обращения, но и прочих переменных) следует считать достаточно близким к предыдущему запросу, чтобы можно было использовать результат из кэша?

**Плохо**:

```
// Возвращает цену лунго в кафе,
// ближайшем к указанной точке
GET /v1/price?recipe=lungo↵
  &longitude={longitude}↵
  &latitude={latitude}
→
{ "currency_code", "price" }
```

Возникает два вопроса:

- в течение какого времени эта цена действительна?
- на каком расстоянии от указанной точки цена всё ещё действительна?

**Хорошо**: Для указания времени жизни кэша можно пользоваться стандартными средствами протокола, например, заголовком `Cache-Control`. В ситуации, когда кэш существует не только во временном измерении (как, например, в нашем примере добавляется пространственное измерение), вам придётся разработать свой формат описания параметров кэширования.

```
GET /v1/price?recipe=lungo↵
  &longitude={longitude}↵
  &latitude={latitude}
→
{
  "offer": {
    "id",
    "currency_code",
    "price",
    "conditions": {
      // До какого времени
      // валидно предложение
      "valid_until",
      // Где валидно предложение:
      // * город
      // * географический объект
      // * …
      "valid_within"
    }
  }
}
```

**NB**: часто можно встретить подход, когда для неизменяемых данных выставляется очень длинный срок жизни кэша — год, а то и больше. С практической точки зрения это не имеет большого смысла (вряд ли можно всерьёз ожидать серьёзного снижения нагрузки на сервер по сравнению, скажем, с кэшированием на месяц), а вот цена ошибки существенно возрастает: если по какой-то причине будут закэшированы неверные данные (например, ошибка `404`), эта проблема будет преследовать вас следующий год, а то и больше. Мы склонны рекомендовать выбирать разумные сроки кэширования в зависимости от того, насколько серьёзным окажется для бизнеса кэширование неверного значения.

##### [22\. Сохраняйте точность дробных чисел](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-22)

Там, где это позволено протоколом, дробные числа с фиксированной запятой — такие, как денежные суммы, например — должны передаваться в виде специально предназначенных для этого объектов, например, `Decimal` или аналогичных.

Если в протоколе нет `Decimal`\-типов (в частности, в JSON нет чисел с фиксированной запятой), следует либо привести к целому (путём домножения на указанный множитель), либо использовать строковый тип.

Если конвертация в формат с плавающей запятой заведомо приводит к потере точности (например, если мы переведём 20 минут в часы в виде десятичной дроби), то следует либо предпочесть формат без потери точности (т.е. предпочесть формат `00:20` формату `0.333333…`), либо предоставить SDK работы с такими данными, либо (в крайнем случае) описать в документации принципы округления.

##### [23\. Все операции должны быть идемпотентны](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-23)

Напомним, идемпотентность — это следующее свойство: повторный вызов той же операции с теми же параметрами не изменяет результат. Поскольку мы обсуждаем в первую очередь клиент-серверное взаимодействие, узким местом в котором является ненадежность сетевой составляющей, повтор запроса при обрыве соединения — не исключительная ситуация, а норма жизни.

Там, где идемпотентность не может быть обеспечена естественным образом, необходимо добавить явный параметр — ключ идемпотентности или ревизию.

**Плохо**:

```
// Создаёт заказ
POST /orders
```

Повтор запроса создаст два заказа!

**Хорошо**:

```
// Создаёт заказ
POST /v1/orders
X-Idempotency-Token: <случайная строка>
```

Клиент на своей стороне запоминает `X-Idempotency-Token`, и, в случае автоматического повторного перезапроса, обязан его сохранить. Сервер на своей стороне проверяет токен и, если заказ с таким токеном уже существует для этого клиента, не даёт создать заказ повторно.

**Альтернатива**:

```
// Создаёт черновик заказа
POST /v1/orders/drafts
→
{ "draft_id" }
```

```
// Подтверждает черновик заказа
PUT /v1/orders/drafts↵
  /{draft_id}/confirmation
{ "confirmed": true }
```

Создание черновика заказа — необязывающая операция, которая не приводит ни к каким последствиям, поэтому допустимо создавать черновики без токена идемпотентности. Операция подтверждения заказа — уже естественным образом идемпотентна, для неё `draft_id` играет роль ключа идемпотентности.

**Другая альтернатива** — использование техник оптимистичного управления параллелизмом, о которых мы поговорим в главе «[Стратегии синхронизации](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies)».

Также стоит упомянуть, что добавление токенов идемпотентности к эндпойнтам, которые и так изначально идемпотентны, имеет определённый смысл, так как токен помогает различить две ситуации:

- клиент не получил ответ из-за сетевых проблем и пытается повторить запрос;
- клиент ошибся, пытаясь применить конфликтующие изменения.

Рассмотрим следующий пример: представим, что у нас есть ресурс с общим доступом, контролируемым посредством номера ревизии, и клиент пытается его обновить.

```
POST /resource/updates
{
  "resource_revision": 123
  "updates"
}
```

Сервер извлекает актуальный номер ревизии и обнаруживает, что он равен 124. Как ответить правильно? Можно просто вернуть `409 Conflict`, но тогда клиент будет вынужден попытаться выяснить причину конфликта и как-то решить его, потенциально запутав пользователя. К тому же, фрагментировать алгоритмы разрешения конфликтов, разрешая каждому клиенту реализовать какой-то свой — плохая идея.

Сервер мог бы попытаться сравнить значения поля `updates`, предполагая, что одинаковые значения означают перезапрос, но это предположение будет опасно неверным (например, если ресурс представляет собой счётчик, то последовательные запросы с идентичным телом нормальны).

Добавление токена идемпотентности (явного в виде случайной строки или неявного в виде черновиков) решает эту проблему:

```
POST /resource/updates
X-Idempotency-Token: <токен>
{
  "resource_revision": 123
  "updates"
}
→ 201 Created
```

— сервер обнаружил, что ревизия 123 была создана с тем же токеном идемпотентности, а значит клиент просто повторяет запрос.

Или:

```
POST /resource/updates
X-Idempotency-Token: <токен>
{
  "resource_revision": 123
  "updates"
}
→ 409 Conflict
```

— сервер обнаружил, что ревизия 123 была создана с другим токеном, значит имеет место быть конфликт общего доступа к ресурсу.

Более того, добавление токена идемпотентности не только решает эту проблему, но и позволяет в будущем сделать продвинутые оптимизации. Если сервер обнаруживает конфликт общего доступа, он может попытаться решить его, «перебазировав» обновление, как это делают современные системы контроля версий, и вернуть `200 OK` вместо `409 Conflict`. Эта логика существенно улучшает пользовательский опыт и при этом полностью обратно совместима и предотвращает фрагментацию кода разрешения конфликтов.

Но имейте в виду: клиенты часто ошибаются при имплементации логики токенов идемпотентности. Две проблемы проявляются постоянно:

- нельзя полагаться на то, что клиенты генерируют честные случайные токены — они могут иметь одинаковый seed рандомизатора или просто использовать слабый алгоритм или источник энтропии; при проверке токенов нужны слабые ограничения: уникальность токена должна проверяться не глобально, а только применительно к конкретному пользователю и конкретной операции;
- клиентские разработчики могут неправильно понимать концепцию — или генерировать новый токен на каждый перезапрос (что на самом деле неопасно, в худшем случае деградирует UX), или, напротив, использовать один токен для разнородных запросов (а вот это опасно и может привести к катастрофически последствиям; ещё одна причина имплементировать совет из предыдущего пункта!); поэтому рекомендуется написать хорошую документацию и/или клиентскую библиотеку для перезапросов.

##### [24\. Не изобретайте безопасность](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-24)

Если бы автору этой книги давали доллар каждый раз, когда ему приходилось бы имплементировать кем-то придуманный дополнительный протокол безопасности — он бы давно уже был на заслуженной пенсии. Любовь разработчиков API к подписыванию параметров запросов или сложным схемам обмена паролей на токены столь же несомненна, сколько и бессмысленна.

**Во-первых**, почти всегда процедуры, обеспечивающие безопасность той или иной операции, _уже разработаны_. Нет никакой нужды придумывать их заново, просто имплементируйте какой-то из существующих протоколов. Никакие самописные алгоритмы проверки сигнатур запросов не обеспечат вам того же уровня защиты от атаки Manipulator-in-the-middle (_MitM_)<sup><a id="api-design-describing-interfaces-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-3">3</a></sup>, как соединение по протоколу TLS с взаимной проверкой сигнатур сертификатов<sup><a id="api-design-describing-interfaces-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-4">4</a></sup>.

**Во-вторых**, чрезвычайно самонадеянно (и опасно) считать, что вы разбираетесь в вопросах безопасности. Новые вектора атаки появляются каждый день, и быть в курсе всех актуальных проблем — это само по себе работа на полный рабочий день. Если же вы полный рабочий день занимаетесь чем-то другим, спроектированная вами система защиты наверняка будет содержать уязвимости, о которых вы просто никогда не слышали — например, ваш алгоритм проверки паролей может быть подвержен атаке по времени<sup><a id="api-design-describing-interfaces-ref-5-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-5">5</a></sup>, а веб-сервер — атаке с разделением запросов<sup><a id="api-design-describing-interfaces-ref-6-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-6">6</a></sup>.

Фонд OWASP каждый год составляет список самых распространённых уязвимостей в API<sup><a id="api-design-describing-interfaces-ref-7-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-7">7</a></sup>, который мы настоятельно рекомендуем изучить.

Отдельно уточним: любые API должны предоставляться строго по протоколу TLS версии не ниже 1.2 (лучше 1.3).

##### [25\. Помогайте партнёрам не изобретать безопасность](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-25)

Не менее важно не только обеспечивать безопасность API как такового, но и предоставить партнёрам такие интерфейсы, которые минимизируют возможные проблемы с безопасностью на их стороне.

**Плохо**:

```
// Позволяет партнёру задать
// описание для своего напитка
PUT /v1/partner-api/{partner-id}↵
  /recipes/lungo/info
"<script>alert(document.cookie)</script>"
```

```
// возвращает описание
GET /v1/partner-api/{partner-id}↵
  /recipes/lungo/info
→
"<script>alert(document.cookie)</script>"
```

Подобный интерфейс является прямым способом соорудить хранимую XSS, которым потенциально может воспользоваться злоумышленник. Да, это ответственность самого партнёра — не допускать сохранения подобного ввода и/или экранировать его при выводе. Но большие цифры по-прежнему работают против вас: всегда найдутся начинающие разработчики, которые не знают об этом виде уязвимости или не подумали о нём. В худшем случае существование таких хранимых XSS может затронуть не только конкретного партнёра, но и вообще всех пользователей API.

В таких ситуациях мы рекомендуем, во-первых, экранировать вводимые через API данные, если они выглядят потенциально эксплуатируемыми (предназначены для показа в UI и/или возвращаются по прямой ссылке), и, во-вторых, ограничивать радиус взрыва так, чтобы через уязвимости в коде одного партнёра нельзя было затронуть других партнёров. В случае, если функциональность небезопасного ввода всё же нужна, необходимо предупреждать о рисках максимально явно.

**Лучше** (но не идеально):

```
// Позволяет партнёру задать
// потенциально небезопасное
// описание для своего напитка
PUT /v1/partner-api/{partner-id}↵
  /recipes/lungo/info
X-Dangerously-Disable-Sanitizing: true
"<script>alert(document.cookie)</script>"
```

```
// возвращает потенциально
// небезопасное описание
GET /v1/partner-api/{partner-id}↵
  /recipes/lungo/info
X-Dangerously-Allow-Raw-Value: true
→
"<script>alert(document.cookie)</script>"
```

В частности, если вы позволяете посредством API выполнять какие-то текстовые скрипты, всегда предпочитайте безопасный ввод небезопасному.

**Плохо**:

```
POST /v1/run/sql
{
  // Передаёт готовый запрос целиком
  "query": "INSERT INTO data (name)↵
    VALUES ('Robert');↵
    DROP TABLE students;--')"
}
```

**Лучше**:

```
POST /v1/run/sql
{
  // Передаёт шаблон запроса
  "query": "INSERT INTO data (name)↵
    VALUES (?)",
  // и параметры для подстановки
  "values": [
    "Robert');↵
     DROP TABLE students;--"
  ]
}
```

Во втором случае вы сможете централизованно экранировать небезопасный ввод и избежать тем самым SQL-инъекции. Напомним повторно, что делать это необходимо с помощью state-of-the-art инструментов, а не самописных регулярных выражений.

##### [26\. Используйте глобально уникальные идентификаторы](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-26)

Хорошим тоном при разработке API будет использование для идентификаторов сущностей глобально уникальных строк, либо семантичных (например, "lungo" для видов напитков), либо случайных (например UUID-4<sup><a id="api-design-describing-interfaces-ref-8-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-8">8</a></sup>). Это может чрезвычайно пригодиться, если вдруг придётся объединять данные из нескольких источников под одним идентификатором.

Мы вообще склонны порекомендовать использование идентификаторов в urn-подобном формате, т.е. `urn:order:<uuid>` (или просто `order:<uuid>`), это сильно помогает с отладкой legacy-систем, где по историческим причинам есть несколько разных идентификаторов для одной и той же сущности, в таком случае неймспейсы в urn помогут быстро понять, что это за идентификатор и нет ли здесь ошибки использования.

Отдельное важное следствие: **не используйте инкрементальные номера как внешние идентификаторы**. Помимо вышесказанного, это плохо ещё и тем, что ваши конкуренты легко смогут подсчитать, сколько у вас в системе каких сущностей и тем самым вычислить, например, точное количество заказов за каждый день наблюдений.

##### [27\. Предусмотрите ограничения доступа](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-27)

С ростом популярности API вам неизбежно придётся внедрять технические средства защиты от недобросовестного использования — такие, как показ капчи, расстановка приманок-honeypot-ов, возврат ошибок вида «слишком много запросов», постановка прокси-защиты от DDoS перед эндпойнтами и так далее. Всё это невозможно сделать, если вы не предусмотрели такой возможности изначально, а именно — не ввели соответствующей номенклатуры ошибок и предупреждений.

Вы не обязаны с самого начала такие ошибки действительно генерировать — но вы можете предусмотреть их на будущее. Например, вы можете описать ошибку `429 Too Many Requests` или перенаправление на показ капчи, но не имплементировать возврат таких ответов, пока не возникнет в этом необходимость.

Отдельно необходимо уточнить, что в тех случаях, когда через API можно совершать платежи, ввод дополнительных факторов аутентификации пользователя (через TOTP, SMS или технологии типа 3D-Secure) должен быть предусмотрен обязательно.

**NB**: из этого пункта вытекает достаточно очевидное правило, которое, тем не менее, часто нарушают разработчики API — **всегда разделяйте эндпойнты разных семейств API**. Если вы предоставляете и серверное API, и сервисы для конечных пользователей, и виджеты для встраивания в сторонние приложения — эти API должны обслужиться с разных эндпойнтов для того, чтобы вы могли вводить разные меры безопасности (скажем, API-ключи, требование логина и капчу, соответственно).

##### [28\. Не предоставляйте endpoint-ов массового получения чувствительных данных](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-28)

Если через API возможно получение персональных данных, номер банковских карт, переписки пользователей и прочей информации, раскрытие которой нанесёт большой ущерб пользователям, партнёрам и/или вам — методов массового получения таких данных в API быть не должно, или, по крайней мере, на них должны быть ограничения на частоту запросов, размер страницы данных, а в идеале ещё и многофакторная аутентификация.

Часто разумной практикой является предоставление таких массовых выгрузок по запросу, т.е. фактически в обход API.

##### [29\. Локализация и интернационализация](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-para-29)

Все эндпойнты должны принимать на вход языковые параметры (например, в виде заголовка `Accept-Language`), даже если на текущем этапе нужды в локализации нет.

Важно понимать, что язык пользователя и юрисдикция, в которой пользователь находится — разные вещи. Цикл работы вашего API всегда должен хранить локацию пользователя. Либо она задаётся явно (в запросе указываются географические координаты), либо неявно (первый запрос с географическими координатами инициировал создание сессии, в которой сохранена локация) — но без локации корректная локализация невозможна. В большинстве случаев локацию допустимо редуцировать до кода страны.

Дело в том, что множество параметров, потенциально влияющих на работу API, зависят не от языка, а именно от расположения пользователя. В частности, правила форматирования чисел (разделители целой и дробной частей, разделители разрядов) и дат, первый день недели, раскладка клавиатуры, система единиц измерения (которая к тому же может оказаться не десятичной!) и так далее. В некоторых ситуациях необходимо хранить две локации: та, в которой пользователь находится, и та, которую пользователь сейчас просматривает. Например, если пользователь из США планирует туристическую поездку в Европу, то цены ему желательно показывать в местной валюте, но отформатированными согласно правилам американского письма.

Следует иметь в виду, что явной передачи локации может оказаться недостаточно, поскольку в мире существуют территориальные конфликты и спорные территории. Каким образом API должен себя вести при попадании координат пользователя на такие территории — вопрос, к сожалению, в первую очередь юридический. Автору этой книги приходилось как-то разрабатывать API, в котором пришлось вводить концепцию «территория государства A по мнению официальных органов государства Б».

**Важно**: различайте локализацию для конечного пользователя и локализацию для разработчика. В примерах выше сообщение `localized_message` адресовано пользователю — его должно показать приложение, если в коде обработка такой ошибки не предусмотрена. Это сообщение должно быть написано на указанном в запросе языке и отформатировано согласно правилам локации пользователя. А вот сообщение `details.checks_failed[].message` написано не для пользователя, а для разработчика, который будет разбираться с проблемой. Соответственно, написано и отформатировано оно должно быть понятным для разработчика образом — что, скорее всего, означает «на английском языке», т.к. английский де-факто является стандартом в мире разработки программного обеспечения.

Следует отметить, что индикация, какие сообщения следует показать пользователю, а какие написаны для разработчика, должна, разумеется, быть явной конвенцией вашего API. В примере для этого используется префикс `localized_`.

И ещё одна вещь: все строки должны быть в кодировке UTF-8 и никакой другой.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-1-back) Законы де Моргана

    [ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD%D1%8B\_%D0%B4%D0%B5\_%D0%9C%D0%BE%D1%80%D0%B3%D0%B0%D0%BD%D0%B0](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD%D1%8B_%D0%B4%D0%B5_%D0%9C%D0%BE%D1%80%D0%B3%D0%B0%D0%BD%D0%B0)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-2-back) Hrala, J. Welcome to Null Island, The Most 'Visited' Place on Earth That Doesn't Actually Exist

    [www.sciencealert.com/welcome-to-null-island-the-most-visited-place-that-doesn-t-exist](https://www.sciencealert.com/welcome-to-null-island-the-most-visited-place-that-doesn-t-exist)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-3-back) Manipulator-in-the-middle Attack

    [owasp.org/www-community/attacks/Manipulator-in-the-middle_attack](https://owasp.org/www-community/attacks/Manipulator-in-the-middle_attack)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-4-back) Mutual Authentication. mTLS

    [en.wikipedia.org/wiki/Mutual_authentication#mTLS](https://en.wikipedia.org/wiki/Mutual_authentication#mTLS)

- [<sup>5</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-5-back) Timing Attack

    [en.wikipedia.org/wiki/Timing_attack](https://en.wikipedia.org/wiki/Timing_attack)

- [<sup>6</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-6-back) HTTP Request Splitting

    [capec.mitre.org/data/definitions/105.html](https://capec.mitre.org/data/definitions/105.html)

- [<sup>7</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-7-back) OWASP API Security Project

    [owasp.org/www-project-api-security](https://owasp.org/www-project-api-security/)

- [<sup>8</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces-ref-8-back) UUID-4

    [en.wikipedia.org/wiki/Universally_unique_identifier#Version_4\_(random)](<https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_(random)>)

## [Раздел II. Паттерны дизайна API](https://twirl.github.io/The-API-Book/API.ru.html#-ii-api)

### [Глава 16. Стратегии синхронизации](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies)

Перейдём теперь к техническим проблемам, стоящим перед разработчикам API, и начнём с последней из описанных во вводной главе — необходимости синхронизировать состояния. Представим, что конечный пользователь размещает заказ на приготовление кофе через наш API. Пока этот запрос путешествует от клиента в кофейню и обратно, многое может произойти. Например, рассмотрим следующую последовательность событий:

1.  Клиент отправляет запрос на создание нового заказа.
2.  Из-за сетевых проблем запрос идёт до сервера очень долго, а клиент получает таймаут:
    - клиент, таким образом, не знает, был ли выполнен запрос или нет.

3.  Клиент запрашивает текущее состояние системы и получает пустой ответ, поскольку таймаут случился раньше, чем запрос на создание заказа дошёл до сервера:

    ```
    let pendingOrders = await
      api.getOngoingOrders(); // → []
    ```

4.  Сервер, наконец, получает запрос на создание заказа и исполняет его.
5.  Клиент, не зная об этом, создаёт заказ повторно.

Поскольку действия чтения списка актуальных заказов и создания нового заказа разнесены во времени, мы не можем гарантировать, что между этими запросами состояние системы не изменилось. Если же мы хотим такую гарантию дать, нам нужно обеспечить какую-то из стратегий синхронизации<sup><a id="api-patterns-sync-strategies-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-1">1</a></sup>. Если в случае, скажем, API операционных систем или клиентских фреймворков мы можем воспользоваться предоставляемыми платформой примитивами, то в кейсе распределённых сетевых API такой примитив нам придётся разработать самостоятельно.

Существуют два основных подхода к решению этой проблемы — пессимистичный (программная реализация блокировок) и оптимистичный (версионирование ресурсов).

**NB**: вообще, лучший способ избежать проблемы — не иметь её вовсе. Если ваш API идемпотентен, то никакой повторной обработки запроса не будет происходить. Однако не все операции в реальном мире идемпотентны в принципе: например, создание нового заказа такой операцией не является. Мы можем добавлять механики, предотвращающие _автоматические_ перезапросы (такие как, например, генерируемый клиентом токен идемпотентности), но не можем запретить пользователю просто взять и повторно создать точно такой же заказ.

#### Программные блокировки

Первый подход — очевидным образом перенести стандартные примитивы синхронизации на уровень API. Например, вот так:

```
let lock;
try {
  // Захватываем право
  // на эксклюзивное исполнение
  // операции создания заказа
  lock = await api.acquireLock(ORDER_CREATION);
  // Получаем текущий список
  // заказов, известных системе
  let pendingOrders = await
    api.getPendingOrders();
  // Если нашего заказа ещё нет,
  // создаём его
  if (pendingOrders.length == 0) {
    let order = await api.createOrder(…)
  }
} catch (e) {
  // Обработка ошибок
} finally {
  // Разблокировка
  await lock.release();
}
```

Достаточно очевидно, что подобного рода подход крайне редко реализуется в распределённых сетевых API, из-за комплекса связанных проблем:

1.  Ожидание получения блокировки вносит во взаимодействие дополнительные плохо предсказуемые и, в худшем случае, весьма длительные задержки.
2.  Сама по себе блокировка — это ещё одна сущность, для работы с которой нужно иметь отдельную весьма производительную подсистему, поскольку для работы блокировок требуется ещё и обеспечить сильную консистентность<sup><a id="api-patterns-sync-strategies-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-2">2</a></sup> в API: метод `getPendingOrders` должен вернуть актуальное состояние системы, иначе повторный заказ всё равно будет создан.
3.  Поскольку клиентская часть разрабатывается сторонними партнёрами, мы не можем гарантировать, что написанный ими код корректно работает с блокировками; неизбежно в системе появятся «висящие» блокировки, а, значит, придётся предоставлять партнёрам инструменты для отслеживания и отладки возникающих проблем.
4.  Необходимо разработать достаточную гранулярность блокировок, чтобы партнёры не могли влиять на работоспособность друг друга. Хорошо, если мы можем ограничить блокировку, скажем, конкретным конечным пользователем в конкретной системе партнёра; но если этого сделать не получается (например, если система авторизации общая и все партнёры имеют доступ к одному и тому же профилю пользователя), то необходимо разрабатывать ещё более комплексные системы, которые будут исправлять потенциальные ошибки в коде партнёров — например, вводить квоты на блокировки.

#### Оптимистичное управление параллелизмом

Более щадящий с точки зрения сложности имплементации вариант — это реализовать оптимистичное управление параллелизмом<sup><a id="api-patterns-sync-strategies-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-3">3</a></sup> и потребовать от клиента передавать признак того, что он располагает актуальным состоянием разделяемого ресурса.

```
// Получаем состояние
let orderState =
  await api.getOrderState();
// Частью состояния является
// версия ресурса
let version =
  orderState.latestVersion;
// Заказ можно создать,
// только если версия состояния
// не изменилась с момента чтения
try {
  let task = await api
    .createOrder(version, …);
} catch (e) {
  // Если версия неверна, т.е. состояние
  // было параллельно изменено
  // другим клиентом, произойдёт ошибка
  if (Type(e) == INCORRECT_VERSION) {
    // Которую нужно как-то обработать…
  }
}
```

**NB**: внимательный читатель может возразить нам, что необходимость имплементировать стратегии синхронизации и строгую консистентность никуда не пропала, т.к. где-то в системе должен существовать компонент, осуществляющий блокирующее чтение версии с её последующим изменением. Это не совсем так: стратегии синхронизации и строгая консистентность _пропали из публичного API_. Расстояние между клиентом, устанавливающим блокировку, и сервером, её обрабатывающим, стало намного меньше, и всё взаимодействие теперь происходит в контролируемой среде (это вообще может быть одна подсистема, если мы используем ACID-совместимую базу данных<sup><a id="api-patterns-sync-strategies-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-4">4</a></sup> или вовсе держим состояние ресурса в оперативной памяти).

Вместо версий можно использовать дату последней модификации ресурса (что в целом гораздо менее надёжно ввиду неидеальной синхронизации часов в разных узлах системы; не забывайте, как минимум, сохранять дату с максимально доступной точностью!) либо идентификаторы сущности (ETag).

Достоинством оптимистичного управления параллелизмом является, таким образом, возможность «спрятать» сложную в имплементации и масштабировании часть «под капотом». Недостаток же состоит в том, что ошибки версионирования теперь являются штатным поведением, и клиентам _придётся_ написать правильную работу с ними, иначе их приложение может вообще оказаться неработоспособным — пользователь будет вечно пытаться создать заказ с неактуальной версией.

**NB**. Выбор ресурса, версию которого мы требуем передать для получения доступа, очень важен. Если в нашем примере мы заведём глобальную версию всей системы, которая изменяется при поступлении любого заказа, то, очевидно, у пользователя будут околонулевые шансы успешно разместить заказ.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-1-back) Synchronization (Computer Science)

    [en.wikipedia.org/wiki/Synchronization\_(computer_science)](<https://en.wikipedia.org/wiki/Synchronization_(computer_science)>)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-2-back) Strong consistency

    [en.wikipedia.org/wiki/Strong_consistency](https://en.wikipedia.org/wiki/Strong_consistency)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-3-back) Optimistic Concurrency Control

    [en.wikipedia.org/wiki/Optimistic_concurrency_control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies-ref-4-back) ACID

    [en.wikipedia.org/wiki/ACID](https://en.wikipedia.org/wiki/ACID)

### [Глава 19. Списки и организация доступа к ним](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-lists)

В предыдущей главе мы пришли вот к такому интерфейсу, позволяющему минимизировать коллизии при создании заказов:

```
let pendingOrders = await api
  .getOngoingOrders();
→
{ orders: [{
  order_id: <идентификатор задания>,
  status: "new"
}, …]}
```

Внимательный читатель может подметить, что этот интерфейс нарушает нашу же рекомендацию, данную в главе [«Описание конечных интерфейсов»](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces): количество возвращаемых данных в любом ответе должно быть ограничено, но в нашем интерфейсе отсутствуют какие-либо лимиты. Эта проблема существовала и в предыдущих версиях этого эндпойнта, но отказ от синхронного создания заказа её усугубил: операция создания задания должна работать максимально быстро, и, следовательно, почти все проверки лимитов мы должны проводить асинхронно — а значит, клиент потенциально может создать очень много заданий, что может многократно увеличить размер ответа функции `getOngoingOrders`.

**NB**: конечно, не иметь _вообще никакого_ ограничения на создание заданий — не самое мудрое решение; какие-то легковесные проверки лимитов должны быть в API. Тем не менее, в рамках этой главы мы фокусируемся именно на проблеме размера ответа сервера.

Исправить эту проблему достаточно просто — можно ввести лимит записей и параметры фильтрации и сортировки, например так:

```
api.getOngoingOrders({
  // необязательное, но имеющее
  // значение по умолчанию
  "limit": 100,
  "parameters": {
    "order_by": [{
      "field": "created_iso_time",
      "direction": "desc"
    }]
  }
})
```

Однако введение лимита ставит другой вопрос: если всё же количество записей, которые нужно выбрать, превышает лимит, каким образом клиент должен получить к ним доступ?

Стандартный подход к этой проблеме — введение параметра `offset` или номера страницы данных:

```
api.getOngoingOrders({
  // необязательное, но имеющее
  // значение по умолчанию
  "limit": 100,
  // По умолчанию — 0
  "offset": 100
  "parameters"
});
```

Однако, как нетрудно заметить, в нашем случае этот подход приведёт к новым проблемам. Пусть для простоты в системе от имени пользователя выполняется три заказа:

```
[{
  "id": 3,
  "created_iso_time": "2022-12-22T15:35",
  "status": "new"
}, {
  "id": 2,
  "created_iso_time": "2022-12-22T15:34",
  "status": "new"
}, {
  "id": 1,
  "created_iso_time": "2022-12-22T15:33",
  "status": "new"
}]
```

Приложение партнёра запросило первую страницу списка заказов:

```
api.getOrders({
  "limit": 2,
  "parameters": {
    "order_by": [{
      "field": "created_iso_time",
      "direction": "desc"
    }]
  }
})
→
{
  "orders": [{
    "id": 3, …
  }, {
    "id": 2, …
  }]
}
```

Теперь приложение запрашивает вторую страницу `"limit": 2, "offset": 2` и ожидает получить заказ `"id": 1`. Предположим, однако, что за время, прошедшее с момента первого запроса, в системе появился новый заказ с `"id": 4`.

```
[{
  "id": 4,
  "created_iso_time": "2022-12-22T15:36",
  "status": "new"
}, {
  "id": 3,
  "created_iso_time": "2022-12-22T15:35",
  "status": "new"
}, {
  "id": 2,
  "created_iso_time": "2022-12-22T15:34",
  "status": "ready"
}, {
  "id": 1,
  "created_iso_time": "2022-12-22T15:33",
  "status": "new"
}]
```

Тогда, запросив вторую страницу заказов, вместо одного заказа `"id": 1`, приложение партнёра получит повторно заказ `"id": 2`:

```
api.getOrders({
  "limit": 2,
  "offset": 2
  "parameters"
})
→
{
  "orders": [{
    "id": 2, …
  }, {
    "id": 1, …
  }]
}
```

Такие перестановки крайне неудобны и для пользовательских интерфейсов — если, допустим, предположить, что заказы запрашивает бухгалтер партнёра, чтобы рассчитать выплаты, то и он легко может просто не заметить, что какой-то заказ посчитан дважды. Однако в случае _программной_ интеграции ситуация становится намного сложнее: разработчику приложения нужно написать достаточно неочевидный код (сохраняющий состояние уже полученных страниц данных), чтобы провести такой перебор корректно.

Отметим теперь, что ситуацию легко можно сделать гораздо более запутанной. Например, если мы добавим сортировку не только по дате создания, но и по статусу заказа:

```
api.getOrders({
  "limit": 2,
  "parameters": {
    "order_by": [{
      "field": "status",
      "direction": "desc"
    }, {
      "field": "created_iso_time",
      "direction": "desc"
    }]
  }
})
→
{
  "orders": [{
    "id": 3,
    "status": "new"
  }, {
    "id": 2,
    "status": "new"
  }]
}
```

Предположим, что в интервале между запросами первой и второй страницы заказ `"id": 1` изменил свой статус, и, соответственно, свое положение в списке, став самым первым. Тогда, запросив вторую страницу, приложение партнёра получит (повторно) только заказ с `"id": 2`, а заказ `"id": 1` попросту вообще пропустит, и вновь не будет располагать вообще никаким способом узнать об этом пропуске.

Повторимся, такой подход плохо работает для визуальных интерфейсов, но в программных интерфейсах он практически гарантированно приведёт к ошибкам. **API должно предоставлять способы перебора больших списков, которые гарантируют клиенту получение полного и целостного набора данных**.

Если не вдаваться в детали имплементации, то можно выделить три основных паттерна организации такого перебора — в зависимости от того, как сами по себе организованы данные.

#### Иммутабельные списки

Проще всего организовать доступ, конечно, если список в принципе не может измениться, т.е. все данные в нём фиксированы. Тогда даже схема с `limit`/`offset` прекрасно работает и не требует дополнительных ухищрений. К сожалению, в реальных предметных областях встречается редко.

#### Пополняемые списки, иммутабельные данные

Более распространённый случай — когда не меняются данные в списке, но появляются новые элементы. Чаще всего речь идёт об очередях событий — например, новых сообщений или уведомлений. Представим, что в нашем кофейном API есть эндпойнт для партнёра для получения истории предложений:

```
GET /v1/partners/{id}/offers/history↵
  ?limit=<лимит>
→
{
  "offer_history": [{
    // Идентификатор элемента
    // списка
    "id",
    // Идентификатор пользователя,
    // получившего оффер
    "user_id",
    // Время и дата поиска
    "occurred_at",
    // Установленные пользователем
    // параметры поиска предложений
    "search_parameters",
    // Офферы, которые пользователь
    // увидел
    "offers"
  }]
}
```

Данные в списке по своей природе неизменны — они отражают уже случившийся факт: пользователь искал предложения, и увидел вот такой их список. Но новые элементы списка постоянно возникают, причём вполне могут возникать большими сериями, если пользователь сделал несколько поисков подряд.

Партнёр может использовать эти данные, например, для реализации двух сценариев:

1.  Анализ поведения пользователей в реальном времени (скажем, партнёр может отправить пользователю пуш-уведомление с предложением скидки тем пользователям, которые искали).
2.  Построение статистического отчёта (скажем, подсчёт конверсии по часам).

Для этих сценариев нам необходимо предоставить партнёру две операции со списками:

1.  Для первой задачи, получение в реальном времени всех новых элементов с момента последнего запроса.
2.  Для второй задачи, перебор списка, т.е. получение всех запросов за указанный временной интервал.

Оба сценария покрываются `limit`/`offset`\-схемой, но требуют значительных усилий при написании кода, так как партнёру в обоих случаях нужно как-то ориентироваться, на сколько элементов очередь событий сдвинулась с момента последнего запроса. Отдельно отметим, что использование `limit`/`offset`\-подхода приводит к невозможности кэширования ответов — повторные запросы с той же парой `limit`/`offset` могут возвращать совершенно разные результаты.

Решить эту проблему мы можем, если будем ориентироваться не на позицию элемента в списке (которая может меняться), а на какие-то другие признаки. Нам важно здесь следующее условие: по этому признаку мы можем однозначно определить, какие элементы списка «более новые» по отношению к нему (т.е. имеют меньшие индексы), а какие «более старые».

Если хранилище данных, в котором находятся элементы списка, позволяет использовать монотонно растущие идентификаторы (что на практике означает два условия: (1) база данных поддерживает автоинкрементные колонки, (2) вставка данных осуществляется блокирующим образом), то идентификатор элемента в списке является максимально удобным способом организовать перебор:

```
// Получить записи новее,
// чем запись с указанным id
GET /v1/partners/{id}/offers/history↵
  ?newer_than=<item_id>&limit=<limit>
// Получить записи более старые,
// чем запись с указанным id
GET /v1/partners/{id}/offers/history↵
  ?older_than=<item_id>&limit=<limit>
```

Первый формат запроса позволяет решить задачу (1), т.е. получить все элементы списка, появившиеся позднее последнего известного; второй формат — задачу (2), т.е. перебрать нужно количество записей в истории запросов. Важно, что первый запрос при этом ещё и кэшируемый.

**NB**: отметим, что в главе [«Описание конечных интерфейсов»](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces) мы давали рекомендацию не предоставлять доступ во внешнем API к инкрементальным id. Однако, схема этого и не требует: внешние идентификаторы могут быть произвольными (не обязательно монотонными) — достаточно, чтобы они однозначно конвертировались во внутренние монотонные идентификаторы.

Другим способом организации такого перебора может быть дата создания записи, но этот способ чуть сложнее в имплементации:

- дата создания двух записей может полностью совпадать, особенно если записи могут массово генерироваться программно; в худшем случае может получиться так, что в один момент времени было создано больше записей, чем максимальный лимит их извлечения, и тогда часть записей вообще нельзя будет перебрать;
- если хранилище данных поддерживает распределённую запись, то может оказаться, что более новая запись имеет чуть меньшую дату создания, нежели предыдущая известная (поскольку часы на разных виртуальных машинах могут идти чуть по-разному, и добиться хотя бы микросекундной точности крайне сложно<sup><a id="api-patterns-lists-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-lists-ref-1">1</a></sup>, т.е. нарушится требование монотонности по признаку даты; если использование такого хранилища не имеет альтернативы, необходимо выбрать одно из двух зол:
    - внести рукотворные задержки, т.е. возвращать в API только элементы, созданные более чем N секунд назад — так, чтобы N было заведомо больше неравномерности хода часов (эта техника может использоваться и в тех случаях, когда список формируется асинхронно) — однако надо иметь в виду, что это решение вероятностное и всегда есть шанс отдачи неверных данных в случае проблем с синхронизацией на бэкенде;
    - описать нестабильность порядка новых элементов списка в документации и переложить решение этой проблемы на партнёров.

Часто подобные интерфейсы перебора данных (путём указания граничного значения) обобщают через введение понятия _курсор_:

```
// Инициализируем поиск
POST /v1/partners/{id}/offers/history↵
  /search
{
  "order_by": [{
    "field": "created",
    "direction": "desc"
  }]
}
→
{
  "cursor": "TmluZSBQcmluY2VzIGluIEFtYmVy"
}
```

```
// Получение порции данных
GET /v1/partners/{id}/offers/history↵
  ?cursor=TmluZSBQcmluY2VzIGluIEFtYmVy↵
  &limit=100
→
{
  "items": […],
  // Указатель на следующую
  // страницу данных
  "cursor": "R3VucyBvZiBBdmFsb24"
}
```

Курсором в данной ситуации может представлять собой просто идентификатор последней записи, а может содержать зашифрованное представление всех параметров поиска. Одним из преимуществ использования абстрактного курсора вместо конкретных монотонных полей является возможность сменить нижележащую технологию (например, перейти от использования последнего известного идентификатора к использованию даты последней известной записи) без слома обратной совместимости. (Поэтому курсоры часто представляют собой «непрозрачные» строки: предоставление читаемых курсоров будет означать, что вы теперь обязаны поддерживать формат курсора, даже если никогда его не документировали. Лучше возвращать курсоры зашифрованными или хотя бы в таком виде, который не вызывал бы желания его раскодировать и поэкспериментировать с параметрами.)

В подходе с курсорами вы сможете без нарушения обратной совместимости добавлять новые фильтры и виды сортировки — при условии, конечно, что вы сможете организовать хранение данных таким образом, чтобы перебор с курсором работал однозначно.

```
// Инициализируем поиск
POST /v1/partners/{id}/offers/history↵
  search
{
  // Добавим фильтр по виду кофе
  "filter": {
    "recipe": "americano"
  },
  // добавим новую сортировку
  // по удалённости от указанной
  // географической точки
  "order_by": [{
    "mode": "distance",
    "location": [-86.2, 39.8]
  }]
}
→
{
  "items": […],
  "cursor":
    "Q29mZmVlIGFuZCBDb250ZW1wbGF0aW9u"
}
```

Небольшое примечание: признаком окончания перебора часто выступает отсутствие курсора на последней странице с данными; мы бы рекомендовали так не делать (т.е. всё же возвращать курсор, указывающий на пустой список), поскольку это позволит добавить функциональность динамической вставки данных в конец списка.

**NB**: в некоторых источниках перебор через идентификаторы / даты создания / курсор, напротив, не рекомендуется по следующей причине: пользователю невозможно показать список страниц и дать возможность выбрать произвольную. Здесь следует отметить, что:

- подобный кейс — список страниц и выбор страниц — существует только для пользовательских интерфейсов; представить себе API, в котором действительно требуется доступ к случайным страницам данных мы можем с очень большим трудом;
- если же мы всё-таки говорим об API приложения, которое содержит элемент управления с постраничной навигацией, то наиболее правильный подход — подготавливать данные для этого элемента управления на стороне сервера, в т.ч. генерировать ссылки на страницы;
- подход с курсором не означает, что `limit`/`offset` использовать нельзя — ничто не мешает сделать двойной интерфейс, который будет отвечать и на запросы вида `GET /items?cursor=…`, и на запросы вида `GET /items?offset=…&limit=…`;
- наконец, если возникает необходимость предоставлять доступ к произвольной странице в пользовательском интерфейсе, то следует задать себе вопрос, какая проблема тем самым решается; вероятнее всего с помощью этой функциональности пользователь что-то ищет: определенный элемент списка или может быть позицию, на которой он закончил работу со списком в прошлый раз; возможно, для этих задач следует предоставить более удобные элементы управления, нежели перебор страниц.

#### Общий сценарий

Увы, далеко не всегда данные организованы таким образом, чтобы из них можно было составить иммутабельные списки. Например, в указанном выше примере поиска текущих заказов мы никак не можем представить постраничный список заказов, находящихся сейчас в статусе «исполняется» — просто потому, что заказы переходят в другие статусы и в реальном времени пропадают из списка. Для таких сложных случаев нам нужно в первую очередь ориентироваться на _сценарии использования_ данных.

Бывает так, что задачу можно _свести_ к иммутабельному списку, если по запросу создавать какой-то слепок запрошенных данных. Во многих случаях работа с таким срезом данных по состоянию на определённую дату более удобна и для партнёров, поскольку снимает необходимость учитывать текущие изменения. Часто такой подход работает с «холодными» хранилищами, которые по запросу выгружают какой-то подмассив данных в «горячее» хранилище.

```
POST /v1/orders/archive/retrieve
{
  "created_iso_date": {
    "from": "1980-01-01",
    "to": "1990-01-01"
  }
}
→
{
  "task_id": <идентификатор
    задания на выгрузку данных>
}
```

Недостаток такого подхода понятен — он требует дополнительных (и зачастую немалых) затрат на создание и хранение слепка, а потому требует и отдельной тарификации. Кроме того, проблема-то сама по себе никуда не делась: мы перенесли её из публичного API на уровень реализации нашего бэкенда, но нам всё ещё нужно каким-то образом перебрать массив данных и сформировать консистентный слепок.

Обратный подход к организации такого перебора — это принципиально не предоставлять больше одной страницы данных. Т.е. партнёр может запросить только «последние» в каком-то смысле записи. Такой подход обычно применяется в одном из трёх случаев:

- если эндпойнт представляет собой поисковый алгоритм, который выбирает наиболее релевантные данные — как мы все отлично знаем, вторая страница поисковой выдачи уже никому не нужна;
- если эндпойнт нужен для того, чтобы _изменить_ данные — например, сервис партнёра достаёт все заказы в статусе `"new"` и переводит в статус «принято к исполнению»; тогда пагинация на самом деле и не нужна, поскольку каждым своим действием партнёр удаляет часть элементов из списка;
    - частный случай такого изменения — просто пометить полученные данные прочитанными;
- наконец, если через эндпойнт предоставляются только «горячие» необработанные данные, а к обработанным данным доступ предоставляется уже через стандартные интерфейсы.

Если ни один из описанных вариантов не подходит по тем или иным причинам, единственный способ организации доступа — это изменение предметной области. Если мы не можем консистентно упорядочить элементы списка, нам нужно найти какой-то другой срез тех же данных, который мы _можем_ упорядочить. Например, в нашем случае доступа к новым заказам мы можем упорядочить _список событий_ создания нового заказа:

```
// Получить все события создания
// заказа, более старые,
// чем запись с указанным id
GET /v1/orders/created-history↵
  ?older_than=<item_id>&limit=<limit>
→
{
  "orders_created_events": [{
    "id": <идентификатор события>,
    "occured_at",
    // Идентификатор заказа
    "order_id"
  }, …]
}
```

События иммутабельны, и их список только пополняется, следовательно, организовать перебор этого списка вполне возможно. Да, событие — это не то же самое, что и сам заказ: к моменту прочтения партнёром события, заказ уже давно может изменить статус. Но, тем не менее, мы предоставили возможность перебрать _все_ новые заказы, пусть и не самым оптимальным образом.

**NB**: в вышеприведённых фрагментах кода мы опустили метаданные ответа — такие как общее число элементов в списке, флаг типа `has_more_items` для индикации необходимости продолжить перебор и т.д. Хотя эти метаданные необязательны (клиент узнает размер списка, когда переберёт его полностью), их наличие повышает удобство работы с API для разработчиков, и мы рекомендуем их добавлять.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-lists-ref-1-back) Ranganathan, K. A Matter of Time: Evolving Clock Sync for Distributed Databases

    [www.yugabyte.com/blog/evolving-clock-sync-for-distributed-databases](https://www.yugabyte.com/blog/evolving-clock-sync-for-distributed-databases/)

## [Раздел IV. HTTP API и архитектурные принципы REST](https://twirl.github.io/The-API-Book/API.ru.html#-iv-http-api-rest)

### [Глава 32. О концепции HTTP API. Парадигмы разработки клиент-серверного взаимодействия](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts)

Вопросы организации HTTP API — к большому сожалению, одни из самых «холиварных». Будучи одной из самых популярных и притом весьма непростых в понимании технологий (ввиду большого по объёму и фрагментированного на отдельные RFC стандарта), спецификация HTTP обречена быть плохо понятой и превратно истолкованной миллионами разработчиков и многими тысячами учебных пособий. Поэтому, прежде чем переходить непосредственно к полезной части настоящего раздела, мы обязаны дать уточнения, о чём же всё-таки пойдёт речь.

Начнём с небольшой исторической справки. Выполнение запросов пользователя на удалённом сервере — одна из базовых задач в программировании ещё со времён мейнфреймов, естественным образом получившая новый импульс развития с появлением ARPANET. Хотя первые высокоуровневые протоколы сетевого взаимодействия работали в терминах отправки по сети сообщений (см., например, протокол DEL предложенный в одном из самых первых RFC — RFC-5 от 1969 года<sup><a id="http-api-concepts-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-1">1</a></sup>), довольно быстро теоретики пришли к мысли, что было бы гораздо удобнее, если бы обращение к удалённому узлу сети и использование удалённых ресурсов _с точки зрения сигнатуры вызова_ ничем не отличались от обращения к локальной памяти и локальным ресурсам. Эту концепцию строго сформулировал под названием Remote Procedure Call (RPC) в 1981 году сотрудник знаменитой лаборатории Xerox в Пало-Альто Брюс Нельсон<sup><a id="http-api-concepts-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-2">2</a></sup> и он же был соавтором первой практической имплементации предложенной парадигмы — Sun RPC<sup><a id="http-api-concepts-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-3">3</a></sup><sup>·<a id="http-api-concepts-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-4">4</a></sup>, существующей и сегодня под названием ONC RPC.

Первые широко распространённые RPC-протоколы — такие как упомянутый Sun RPC, Java RMI<sup><a id="http-api-concepts-ref-5-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-5">5</a></sup>, CORBA<sup><a id="http-api-concepts-ref-6-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-6">6</a></sup> — чётко следовали парадигме. Технология позволила сделать именно то, о чём писал Нельсон — не различать локальное и удалённое исполнение кода. Вся «магия» скрыта внутри обвязки, которая генерирует имплементацию работы с удалённым сервером, а с форматом передачи данных разработчик и вовсе не сталкивается.

Удобство использования технологии, однако, оказалось её же Ахиллесовой пятой:

- требование работы с удалёнными вызовами как с локальными приводит к высокой сложности протокола в силу необходимости поддерживать разнообразные возможности высокоуровневых языков программирования;
- RPC первого поколения диктуют выбор языка и платформы и для клиента, и для сервера:
    - Sun RPC не работал под Windows;
    - Java RMI требовал виртуальную машину Java для работы;
    - некоторые протоколы (CORBA, в частности) декларировали возможность разработки адаптеров для любого языка программирования, однако фактическая имплементация поддержки произвольных языков оказалась весьма сложной;
- проксирование запросов и шардирование данных осложнено необходимостью вычитывать и разбирать тело запроса, что может быть ресурсоёмко;
- возможность адресовать объекты в памяти удалённого сервера так же, как и локальные накладывает также огромные ограничения на масштабирование такой системы;
    - любопытно, что ни один заметный RPC-протокол управление разделяемой памятью не реализовал, но сама _возможность_ в протоколы (в частности, Sun RPC) была заложена.

Одновременно с идеологическим кризисом RPC-подходов первого поколения, который стал особенно заметен с появлением массовых клиент-серверных приложений, где производительность намного важнее удобства разработчиков, происходит и другой процесс — стандартизации сетевых протоколов. Ещё в начале 90-х наблюдалось огромное разнообразие форматов взаимодействия, но постепенно сетевой стек в двух точках практических полностью унифицировался. Одна из них — это Internet protocol suite, состоящий из базового протокола IP и надстройки в виде TCP или UDP над ним. На сегодняшний день альтернативы TCP/IP используются в чрезвычайно ограниченном спектре задач, и средний разработчик практически не сталкивается ни с каким другим сетевым стеком.

Однако у TCP/IP с прикладной точки зрения есть существенный недостаток — он оперирует поверх системы IP-адресов, которые плохо подходят для организации распределённых систем:

- во-первых, люди не запоминают IP-адреса и предпочитают оперировать «говорящими» именами;
- во-вторых, IP-адрес является технической сущностью, связанной с узлом сети, а разработчики хотели бы иметь возможность добавлять и изменять узлы, не нарушая работы своих приложений.

Удобной (и опять же имеющей почти стопроцентное проникновение) абстракцией над IP-адресами оказалась система доменных имён, позволяющая назначить узлам сети человекочитаемые синонимы. Появление доменных имён потребовало разработки клиент-серверных протоколов более высокого, чем TCP/IP, уровня, и для передачи текстовых (гипертекстовых) данных таким протоколом стал HTTP 0.9<sup><a id="http-api-concepts-ref-7-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-7">7</a></sup>, разработанный Тимом Бёрнерсом-Ли и опубликованный в 1991 году. Помимо поддержки обращения к узлам сети по именам, HTTP также предоставил ещё одну очень удобную абстракцию, а именно назначение собственных адресов эндпойнтам, работающим на одном сетевом узле.

Протокол был очень прост и всего лишь описывал способ получить документ, открыв TCP/IP соединение с сервером и передав строку вида `GET адрес_документа`. Позднее протокол был дополнен стандартом URL, позволяющим детализировать адрес документа, и далее протокол начал развиваться стремительно: появились новые глаголы помимо `GET`, статусы ответов, заголовки, типы данных и так далее.

HTTP появился изначально для передачи размеченного гипертекста, что для программных интерфейсов подходит слабо. Однако HTML со временем эволюционировал в более строгий и машиночитаемый XML, который быстро стал одним из общепринятых форматов описания вызовов API. (Забегая вперёд, скажем, что в начале 2000-х XML был быстро вытеснен ещё более простым и интероперабельным JSON.)

Поскольку, с одной стороны, HTTP был простым и понятным протоколом, позволяющим осуществлять произвольные запросы к удаленным серверам по их доменным именам, и, с другой стороны, быстро оброс почти бесконечным количеством разнообразных расширений над базовой функциональностью, он стал второй точкой, к которой сходятся сетевые технологии: практически все запросы к API внутри TCP/IP-сетей осуществляются по протоколу HTTP (и даже если используется альтернативный протокол, запросы в нём всё равно зачастую оформлены в виде HTTP-пакетов просто ради удобства). HTTP, однако, идеологически совершенно противоположен RPC, поскольку не предполагает ни нативной обвязки для функций удалённого вызова, ни тем более разделяемого доступа к памяти. Зато HTTP предложил несколько очень удобных концепций для наращивания производительности серверов, такие как управление кэшированием из коробки и концепцию прозрачных прокси.

В итоге в середине 1990-х годов происходит постепенный отказ от RPC-фреймворков первого поколения в пользу нового подхода, который позднее Рой Филдинг в своей диссертации 2000 года обобщит под названием «Representational State Transfer» или «REST» (о чём мы поговорим чуть позже в соответствующей главе). В новой парадигме отношения между данными и операциями над ними переворачиваются с ног на голову:

- клиент не вызывает процедуры на сервере с передачей параметров — клиент указывает серверу абстрактный адрес (локатор) фрагмента данных (_ресурса_), к которому он хочет применить операцию;
    - сам список операций лимитирован и стандартизирован, семантика их чётко определена в стандарте;
- клиент и сервер независимы и _принципиально_ не имеют никакого разделяемого состояния; все необходимые для исполнения операции параметры должны быть переданы явно;
    - между клиентом и сервером может находиться множество промежуточных узлов сети (прокси и гейтвеев), что не влияет на протокол;
    - если все важные параметры операции (в частности, идентификаторы ресурсов) включены в URL, данные легко можно шардировать без серьёзных затрат;
- сервер размечает параметры кэширования передаваемых данных (представления ресурсов), клиент (и промежуточные прокси) имеют право кэшировать данные в соответствии с разметкой.

**NB**: отказ от архитектур, где клиент и сервер жёстко связаны, в пользу ресурсоориентированных stateless подходов собственно породил понятие дизайна клиент-серверного API, поскольку потребовалось зафиксировать _контракт_ между клиентом и сервером. В парадигме ранних RPC-фреймворков говорить о дизайне API бессмысленно, поскольку написанный разработчиком код и есть API взаимодействия, а с нижележащими протоколами разработчику и вовсе не было нужды знакомиться.

Хотя новый подход оказался весьма удачным с точки зрения разработки высокопроизводительных сервисов, проблемы неудобства работы с декларативными API из императивных языков никуда не делось. К тому же изначально простой стандарт быстро превратился в монстра Франкенштейна, сшитого из десятков разных подстандартов. Не думаем, что сильно ошибёмся, если скажем, что стандарт HTTP целиком со всеми многочисленными дополнительными RFC не знает полностью ни один человек.

Начиная с конца 2010-х годов мы наблюдаем расцвет RPC-технологий нового поколения — или, вернее было бы сказать, комбинированных технологий, которые одновременно и удобны в применении в императивных языках программирования (поставляются с обвязкой, позволяющей эффективно использовать кодогенерацию), и интероперабельны (работают поверх строго стандартизированных протоколов, которые не зависят от конкретного языка программирования), и масштабируемы (абстрагируют понятие ресурса и не предоставляют прямого доступа к памяти сервера).

Фактически, на сегодня _идеологически_ разница между современным API, следующим классическому архитектурному стилю REST, и современным RPC заключается лишь в разметке кэшируемых данных и принципах адресации: в первом случае единицей доступа является ресурс (а параметры операции передаются дополнительно), а во втором — имя операции (а адреса ресурсов, над которыми она выполняется, передаются дополнительно).

Мы рассмотрим конкретные технологии разработки таких API в следующей главе, но отметим здесь важный момент: почти все они (за исключением разве что MQTT) работают поверх протокола HTTP. Так что большинство современных RPC-протоколов _одновременно_ являются HTTP API.

Тем не менее, _обычно_ словосочетание «HTTP API» используется не просто в значении «любой API, использующий протокол HTTP»; говоря «HTTP API» мы _скорее_ подразумеваем, что он используется не как дополнительный третий протокол транспортного уровня, а именно как протокол уровня приложения, то есть составляющие протокола (такие как: URL, заголовки, HTTP-глаголы, статусы ответа, политики кэширования и т.д.) используются в соответствии с их семантикой, определённой в стандартах. _Обычно_ также подразумевается, что в HTTP API используется какой-то из текстовых форматов передачи данных (JSON, XML) для описания вызовов.

В рамках настоящего раздела мы поговорим о дизайне сетевых API, обладающих следующими характеристиками:

- протоколом взаимодействия является HTTP версий 1.1 и выше;
- форматом данных является JSON (за исключением эндпойнтов, специально предназначенных для передачи данных, как правило, файлов, в других форматах);
- в качестве идентификаторов ресурсов используется URL в соответствии со стандартом;
- семантика вызовов HTTP-эндпойнтов соответствует спецификации;
- никакие из веб-стандартов нигде не нарушаются специально.

**Такое API мы будем для краткости называть просто «HTTP API» или «JSON-over-HTTP API»**. Мы понимаем, что такое использование терминологически не полностью корректно, но писать каждый раз «JSON-over-HTTP эндпойнты, утилизирующие семантику, описанную в стандартах HTTP и URL» или «JSON-over-HTTP API, соответствующий архитектурным ограничениям REST» не представляется возможным. Что касается термина «REST API», то как мы покажем дальше, у него нет консистентного определения, поэтому его использования мы также стараемся избегать.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-1-back) RFC-5. DEL

    [datatracker.ietf.org/doc/html/rfc5](https://datatracker.ietf.org/doc/html/rfc5)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-2-back) Nelson, B. J. (1981) Remote procedure call

    [www.semanticscholar.org/paper/Remote-procedure-call-Nelson/c860de40a88090055948b72d04dd79b02195e06b](https://www.semanticscholar.org/paper/Remote-procedure-call-Nelson/c860de40a88090055948b72d04dd79b02195e06b)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-3-back) Birrell, A. D., Nelson, B. J. (1984) Implementing remote procedure calls

    [dl.acm.org/doi/10.1145/2080.357392](https://dl.acm.org/doi/10.1145/2080.357392)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-4-back) RPC: Remote Procedure Call Protocol Specification

    [datatracker.ietf.org/doc/html/rfc1050](https://datatracker.ietf.org/doc/html/rfc1050)

- [<sup>5</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-5-back) Remote Method Invocation (RMI)

    [www.oracle.com/java/technologies/javase/remote-method-invocation-home.html](https://www.oracle.com/java/technologies/javase/remote-method-invocation-home.html)

- [<sup>6</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-6-back) CORBA

    [www.corba.org](https://www.corba.org/)

- [<sup>7</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts-ref-7-back) The Original HTTP as defined in 1991

    [www.w3.org/Protocols/HTTP/AsImplemented.html](https://www.w3.org/Protocols/HTTP/AsImplemented.html)

### [Глава 33. Преимущества и недостатки HTTP API в сравнении с альтернативными технологиями](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons)

Как мы обсудили в предыдущей главе, в настоящий момент выбор технологии для разработки клиент-серверных API сводится к выбору либо ресурсоориентированного подхода (то, что принято называть «REST API», а мы, напомним, будем использовать термин «HTTP API»), либо одного из современных RPC-протоколов. Как мы отмечали, _концептуально_ разница не очень значительна; однако _технически_ разные фреймворки используют протокол HTTP совершенно по-разному:

**Во-первых**, разные фреймворки опираются на разные форматы передаваемых данных:

- HTTP API и некоторые RPC (JSON-RPC<sup><a id="http-api-pros-and-cons-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-1">1</a></sup>, GraphQL<sup><a id="http-api-pros-and-cons-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-2">2</a></sup>) полагаются в основном на формат JSON<sup><a id="http-api-pros-and-cons-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-3">3</a></sup> (опционально дополненный передачей бинарных файлов);
- gRPC<sup><a id="http-api-pros-and-cons-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-4">4</a></sup>, а также Thrift<sup><a id="http-api-pros-and-cons-ref-5-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-5">5</a></sup>, Avro<sup><a id="http-api-pros-and-cons-ref-6-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-6">6</a></sup> и другие специализированные RPC-протоколы полагаются на бинарные форматы (такие как Protocol Buffers<sup><a id="http-api-pros-and-cons-ref-7-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-7">7</a></sup>, FlatBuffers<sup><a id="http-api-pros-and-cons-ref-8-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-8">8</a></sup> и собственный формат Apache Avro);
- наконец, некоторые RPC-протоколы (SOAP<sup><a id="http-api-pros-and-cons-ref-9-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-9">9</a></sup>, XML-RPC<sup><a id="http-api-pros-and-cons-ref-10-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-10">10</a></sup>) используют для передачи данных формат XML<sup><a id="http-api-pros-and-cons-ref-11-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-11">11</a></sup> (что многими разработчиками сегодня воспринимается скорее как устаревшая практика).

**Во-вторых**, существующие реализации различаются подходом к утилизации протокола HTTP:

- либо клиент-серверное взаимодействие опирается на описанные в стандарте HTTP возможности — этот подход характеризует HTTP API;
- либо HTTP утилизируется как транспорт, и поверх него выстроен дополнительный уровень абстракции (т.е. возможности HTTP, такие как номенклатура ошибок или заголовков, сознательно редуцируются до минимального уровня, а вся мета-информация переносится на уровень вышестоящего протокола) — этот подход характерен для RPC протоколов.

У читателя может возникнуть резонный вопрос — а почему вообще существует такая дихотомия: одни API полагаются на стандартную семантику HTTP, другие полностью от неё отказываются в пользу новоизобретённых стандартов, а третьи существуют где-то посередине. Например, если мы посмотрим на формат ответа в JSON-RPC<sup><a id="http-api-pros-and-cons-ref-12-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-12">12</a></sup>, то мы обнаружим, что он легко мог бы быть заменён на стандартные средства протокола HTTP. Вместо

```
HTTP/1.1 200 OK

{
  "jsonrpc": "2.0",
  "id",
  "error": {
    "code": -32600,
    "message": "Invalid request"
  }
}
```

сервер мог бы ответить просто `400 Bad Request` (с передачей идентификатора запроса, ну скажем, в заголовке `X-JSONRPC2-RequestId`). Тем не менее, разработчики протокола посчитали нужным разработать свой собственный формат.

Такая ситуация (не только конкретно с JSON-RPC, а почти со всеми высокоуровневыми протоколами поверх HTTP) сложилась по множеству причин, включая разнообразные исторические (например, невозможность использовать многие возможности HTTP из ранних реализаций `XMLHttpRequest` в браузерах). Однако, новые варианты RPC-протоколов, использующих абсолютный минимум возможностей HTTP, продолжают появляться и сегодня.

Мы можем попытаться выделить по крайней мере три группы причин (помимо идеологических, описанных в предыдущей главе), приводящих к такому размежеванию.

##### [1\. Машиночитаемость метаданных](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-para-1)

Обратим внимание на принципиальное различие между использованием протоколов уровня приложения (как в нашем примере с JSON-RPC) и чистого HTTP. В примере выше ошибку `400 Bad Request` может прочитать практически любой сетевой агент, если мы используем чистый HTTP; если же мы используем собственный формат JSON-RPC, ошибка прозрачной не является — во-первых, потому что понять её может только агент с поддержкой JSON-RPC, а, во-вторых, что более важно, в JSON-RPC статус запроса _не является метаинформацией_. Протокол HTTP позволяет прочитать такие детали, как метод и URL запроса, статус операции, заголовки запроса и ответа, _не читая тело запроса или ответа целиком_. Для большинства протоколов более высокого уровня, включая JSON-RPC, это не так: даже если агент и обладает поддержкой протокола, ему необходимо прочитать и разобрать тело ответа.

Каким образом эта самая возможность читать метаданные может быть полезна? Современный стек взаимодействия между клиентом и сервером является многослойным. Мы можем выделить множество агентов разного уровня, которые, так или иначе, обрабатывают сетевые запросы и ответы:

- разработчик пишет код поверх какого-то фреймворка, который отправляет запросы;
- фреймворк базируется на API языка программирования, компилятор или интерпретатор которого, в свою очередь, полагается на API операционной системы;
- запрос доходит до сервера, возможно, через промежуточные HTTP-прокси;
- сервер, в свою очередь, тоже представляет собой несколько слоёв абстракции в виде фреймворка, языка программирования и ОС;
- перед конечным обработчиком запроса, как правило, находится веб-сервер, проксирующий запрос, а зачастую и не один;
- в современных облачных архитектурах HTTP-запрос, прежде чем дойти до конечного обработчика, пройдёт через несколько абстракций в виде прокси и гейтвеев.

Главное преимущество, которое предоставляет следование букве стандарта HTTP — возможность положиться на то, что промежуточные агенты, от клиентских фреймворков до API-гейтвеев, умеют читать метаданные запроса и выполнять какие-то действия с их использованием — настраивать политику перезапросов и таймауты, логировать, кэшировать, шардировать, проксировать и так далее — без необходимости писать какой-то дополнительный код. Если попытаться сформулировать главный принцип разработки HTTP API, то мы получим примерно следующее: **лучше бы ты разрабатывал API так, чтобы промежуточные агенты могли читать и интерпретировать метаданные запроса и ответа**.

Главным недостатком HTTP API является то, что промежуточные агенты, от клиентских фреймворков до API-гейтвеев, умеют читать метаданные запроса и выполнять какие-то действия с их использованием — настраивать политику перезапросов и таймауты, логировать, кэшировать, шардировать, проксировать и так далее — даже если вы их об этом не просили. Более того, так как стандарты HTTP являются сложными, концепция REST — непонятной, а разработчики программного обеспечения — неидеальными, то промежуточные агенты (и разработчики партнёра!) могут трактовать метаданные запроса _неправильно_. Особенно это касается каких-то экзотических и сложных в имплементации стандартов. Как правило, одной из причин разработки новых RPC-фреймворков декларируется стремление обеспечить простоту и консистентность работы с протоколом, чтобы таким образом уменьшить поле для потенциальных ошибок в реализации интеграции с API.

##### [2\. Качество решений](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-para-2)

Возможность читать и интерпретировать запросы приводит к широкой фрагментации доступных инструментов для работы с HTTP API. На рынке доступно (зачастую бесплатно) множество самых разных инструментов, таких как:

- различные прокси и API-гейтвеи (nginx, Envoy);
- различные форматы описания спецификаций (в первую очередь, OpenAPI) и связанные инструменты для работы со спецификациями (Redoc, Swagger UI) и кодогенерации;
- ПО для разработчиков, позволяющее удобным образом разрабатывать и отлаживать клиенты API (Postman, Insomnia);
- и так далее.

Конечно, большинство этих инструментов применимы и для работы с API, реализующими альтернативные парадигмы. Однако именно способность промежуточных агентов считывать метаданные HTTP запросов и одинаково их интерпретировать позволяет легко строить сложные конвейеры типа экспортировать access-логи nginx в Prometheus и из коробки получить удобные мониторинги статус-кодов ответов в Grafana.

Обратной стороной этой гибкости является качество самих решений и количество времени, необходимого на их интеграцию, особенно если ваш стек чем-то отличается от стандартного. В то же время основной импульс развития альтернативных технологий исходит от какой-то одной крупной IT-компании (Facebook, Google, Apache Software Foundation), которые предоставляют полный набор инструментов для работы со своим протоколом. Такой фреймворк может быть менее функциональным, но почти наверняка является более гомогенным и качественным в плане удобства разработки, поддержки пользователей и количества известных ошибок.

Соображения выше распространяются не только на программное обеспечение, но и на его создателей. Представление разработчиков о HTTP API, увы, также фрагментировано. Практически любой программист как-то умеет работать с HTTP API, но редко при этом досконально знает стандарт или хотя бы консультируется с ним при написании кода. Это ведёт к тому, что добиться качественной и консистентной реализации логики работы с HTTP API может быть сложнее, нежели при использовании альтернативных технологий — причём это соображение справедливо как для партнёров-интеграторов, так и для самого провайдера API.

##### [3\. Вопросы производительности](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-para-3)

В пользу многих современных альтернатив HTTP API — таких как GraphQL, gRPC, Apache Thrift — часто приводят аргумент о низкой производительности JSON-over-HTTP API по сравнению с рассматриваемой технологией; конкретнее, называются следующие проблемы:

1.  Избыточность формата:
    - в JSON необходимо всякий раз передавать имена всех полей, даже если передаётся массив из большого количества одинаковых объектов;
    - большое количество технических символов — кавычек, скобок, запятых — и необходимость экранировать служебные символы в строках.
2.  HTTP API в ответ на запрос к ресурсу возвращает представление ресурса целиком, хотя клиенту могут быть интересны только отдельные поля.
3.  Низкая производительность операций сериализации и десериализации данных.
4.  Передача бинарных данных требует дополнительного кодирования, например, через Base64.
5.  Низкая производительность самого протокола HTTP (в частности, невозможность мультиплексирования нескольких запросов и ответов по одному соединению).

Будем здесь честны: большинство существующих имплементаций HTTP API действительно страдают от указанных проблем. Тем не менее, мы берём на себя смелость заявить, что все эти проблемы большей частью надуманы, и их решению не уделяют большого внимания потому, что указанные накладные расходы не являются сколько-нибудь заметными для большинства вендоров API. В частности:

1.  Если мы говорим об избыточности формата, то необходимо сделать важную оговорку: всё вышесказанное верно, если мы не применяем сжатие. Сравнения показывают<sup><a id="http-api-pros-and-cons-ref-13-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-13">13</a></sup>, что использование gzip практически нивелирует разницу в размере JSON документов относительно альтернативных бинарных форматов (а есть ещё и специально предназначенные для текстовых данных архиваторы, например, brotli<sup><a id="http-api-pros-and-cons-ref-14-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-14">14</a></sup>).
2.  Вообще говоря, если такая нужда появляется, то и в рамках HTTP API вполне можно регулировать список возвращаемых полей ответа, это вполне соответствует духу и букве стандарта. Однако, мы должны заметить, что экономия трафика на возврате частичных состояний (которую мы рассматривали подробно в главе «[Частичные обновления](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-partial-updates)») очень редко бывает оправдана.
3.  Если использовать стандартные десериализаторы JSON, разница по сравнению с бинарными форматами может оказаться действительно очень большой. Если, однако, эти накладные расходы являются проблемой, стоит обратиться к альтернативным десериализаторам — в частности, simdjson<sup><a id="http-api-pros-and-cons-ref-15-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-15">15</a></sup>. Благодаря оптимизированному низкоуровневому коду simdjson показывает отличную производительность, которой может не хватить только совсем уж экзотическим API.
    - Комбинация gzip/brotli + simdjson во многом делает бессмысленным использование в клиент-серверной коммуникации оптимизированных вариантов JSON — таких как, например, BSON<sup><a id="http-api-pros-and-cons-ref-16-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-16">16</a></sup>.

4.  Вообще говоря, парадигма HTTP API подразумевает, что для бинарных данных (такие как изображения или видеофайлы) предоставляются отдельные эндпойнты. Передача бинарных данных в теле JSON-ответа необходима только в случаях, когда отдельный запрос за ними представляет собой проблему с точки зрения производительности. Такой проблемы фактически не существует в server-2-server взаимодействии и в протоколе HTTP 2.0 и выше.
5.  Протокол HTTP 1.1 действительно неидеален с точки зрения мультиплексирования запросов. Однако альтернативные парадигмы организации API для решения этой проблемы опираются… на HTTP версии 2.0. Разумеется, и HTTP API можно построить поверх версии 2.0.

На всякий случай ещё раз уточним: JSON-over-HTTP API _действительно_ проигрывает с точки зрения производительности современным бинарным протоколам. Мы, однако, берём на себя смелость утверждать, что производительность хорошо спроектированного и оптимизированного HTTP API достаточна для абсолютного большинства предметных областей, и реальный выигрыш от перехода на альтернативные протоколы окажется незначителен.

#### Преимущества и недостатки формата JSON

Как нетрудно заметить, большинство претензий, предъявляемых к концепции HTTP API, относятся вовсе не к HTTP, а к использованию формата JSON. В самом деле, ничто не мешает разработать API, которое будет использовать любой бинарный формат вместо JSON (включая те же Protocol Buffers), и тогда разница между Protobuf-over-HTTP API и gRPC сведётся только к (не)использованию подробных URL, статус-кодов и заголовков запросов и ответов (и вытекающей отсюда (не)возможности использовать то или иное стандартное программное обеспечение «из коробки»).

Однако, во многих случаях (включая настоящую книгу) разработчики предпочитают текстовый JSON бинарным Protobuf (Flatbuffers, Thrift, Avro и т.д.) по очень простой причине: JSON очень легко и удобно читать. Во-первых, он текстовый и не требует дополнительной расшифровки; во-вторых, имена полей включены в сам файл. Если сообщение в формате protobuf невозможно прочитать без `.proto`\-файла, то по JSON-документу почти всегда можно попытаться понять, что за данные в нём описаны. В совокупности с тем, что при разработке HTTP API мы также стараемся следовать стандартной семантике всей остальной обвязки, в итоге мы получаем API, запросы и ответы к которому (по крайней мере в теории) удобно читаются и интуитивно понятны.

Помимо человекочитаемости у JSON есть ещё одно важное преимущество: он максимально формален. В нём нет никаких конструкций, которые могут быть по-разному истолкованы в разных архитектурах (с точностью до ограничений на длины чисел и строк), и при этом он удобно ложится в нативные структуры данных (индексные и ассоциативные массивы) почти любого языка программирования. С этой точки зрения у нас фактически не было никакого другого выбора, какой ещё формат данных мы могли бы использовать при написании примеров кода для этой книги.

#### Выбор технологии разработки клиент-серверного API

Как мы видим из вышесказанного, HTTP API и альтернативные RPC-протоколы занимают разные ниши:

- для публичных API предоставление JSON-over-HTTP эндпойнтов является выбором по умолчанию, поскольку эта технология:
    - понятна максимально широкому кругу программистов;
    - позволяет разрабатывать клиентские приложения практически на любой платформе;
- для узкоспециализированных API логично использовать узкоспециализированные фреймворки (например, Apache Avro для работы с Apache Hadoop).

Практика предоставления публичных API в формате, скажем, gRPC, постепенно набирает популярность, но пока ещё незначительна на общем фоне. Таким образом, проблема выбора технологии возникает только для непубличных API общего назначения. На сегодня этот выбор выглядит так:

- HTTP («REST») API;
- gRPC;
- GraphQL;
- множество технологий поменьше, на которых мы не будем останавливаться подробно.

**gRPC** является классической технологией второго поколения, сочетающей все перечисленные выше достоинства:

- полагается на новейшие возможности протокола HTTP/2 и эффективный формат обмена данными Protobuf (последнее необязательно, но практически абсолютное большинство gRPC API полагается именно на него);
- разрабатывается компанией Google и поставляется с широким выбором разнообразных инструментов;
- предлагает contract-first подход, в котором разработка API начинается с написания спецификации;
- благодаря кодогенерации, позволяет удобно работать с протоколом на императивных языках программирования.

К недостаткам gRPC следует отнести:

- сложность декодирования сообщений и отладки коммуникации;
- слабую поддержку браузеров;
- меньшую распространённость (и отсюда более высокий порог вхождения для разработчиков);
- потенциальную зависимость от Google.

В остальном, gRPC вне всяких сомнений один из наиболее современных и производительных протоколов.

**GraphQL** представляет собой любопытный подход, который объединяет концепцию «ресурсов» в HTML (т.е. фокусируется на подробном описании форматов доступных данных и отношений между доменами), при этом предоставляя чрезвычайно богатый язык запросов для извлечения нужных наборов полей. Основная область его применения — это насыщенные разнородными данными предметные области (как, в общем-то, и следует из названия, GraphQL — скорее механизм распределённых запросов к абстрактному хранилищу данных, нежели парадигма разработки API). Предоставление _внешних_ GraphQL API на сегодня скорее экзотика, поскольку с ростом количества данных и запросов GraphQL-сервисом становится очень сложно управлять<sup><a id="http-api-pros-and-cons-ref-17-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-17">17</a></sup>.

**NB**: теоретически, API может обладать двойным интерфейсом, например, и JSON-over-HTTP, и gRPC. Так как в основе всех современных фреймворков лежит формальное описание форматов данных и доступных операций, и эти форматы можно при желании транслировать друг в друга, такой мульти-API технически возможен. Практически же примеры таких API нам неизвестны — по-видимому, накладные расходы на поддержание двойного интерфейса не покрывают потенциальную выгоду от большего удобства для разработчиков.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-1-back) JSON-RPC

    [www.jsonrpc.org](https://www.jsonrpc.org/)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-2-back) GraphQL

    [graphql.org](https://graphql.org/)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-3-back) JSON

    [www.ecma-international.org/publications-and-standards/standards/ecma-404](https://www.ecma-international.org/publications-and-standards/standards/ecma-404/)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-4-back) gRPC

    [grpc.io](https://grpc.io/)

- [<sup>5</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-5-back) Apache Thrift

    [thrift.apache.org](https://thrift.apache.org/)

- [<sup>6</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-6-back) Apache Avro

    [avro.apache.org/docs](https://avro.apache.org/docs/)

- [<sup>7</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-7-back) Protocol Buffers

    [protobuf.dev](https://protobuf.dev/)

- [<sup>8</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-8-back) FlatBuffers

    [flatbuffers.dev](https://flatbuffers.dev/)

- [<sup>9</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-9-back) SOAP

    [www.w3.org/TR/soap12](https://www.w3.org/TR/soap12/)

- [<sup>10</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-10-back) XML-RPC

    [xmlrpc.com](http://xmlrpc.com/)

- [<sup>11</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-11-back) Extensible Markup Language (XML)

    [www.w3.org/TR/xml](https://www.w3.org/TR/xml/)

- [<sup>12</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-12-back) JSON-RPC 2.0 Specification. Response object

    [www.jsonrpc.org/specification#response_object](https://www.jsonrpc.org/specification#response_object)

- [<sup>13</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-13-back) Comparing sizes of protobuf vs json

    [nilsmagnus.github.io/post/proto-json-sizes](https://nilsmagnus.github.io/post/proto-json-sizes/)

- [<sup>14</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-14-back) Brotli Compressed Data Format

    [datatracker.ietf.org/doc/html/rfc7932](https://datatracker.ietf.org/doc/html/rfc7932)

- [<sup>15</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-15-back) simdjson : Parsing gigabytes of JSON per second

    [github.com/simdjson/simdjson](https://github.com/simdjson/simdjson)

- [<sup>16</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-16-back) BSON

    [bsonspec.org](https://bsonspec.org/)

- [<sup>17</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-pros-and-cons-ref-17-back) Mehta, S., Barodiya, K. Lessons learned from running GraphQL at scale

    [blog.dream11engineering.com/lessons-learned-from-running-graphql-at-scale-2ad60b3cefeb](https://blog.dream11engineering.com/lessons-learned-from-running-graphql-at-scale-2ad60b3cefeb)

### [Глава 34. Мифология REST](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth)

Прежде, чем перейти непосредственно к паттернам проектирования HTTP API, мы должны сделать ещё одно терминологическое отступление. Очень часто HTTP API, соответствующие данному нами в главе «[О концепции HTTP API](https://twirl.github.io/The-API-Book/API.ru.html#http-api-concepts)» определению, называют «REST API» или «RESTful API». В настоящем разделе мы эти термины не используем, поскольку оба этих термина неформальные и не несут конкретного смысла.

Что такое «REST»? Как мы упоминали ранее, в 2000 году один из авторов спецификаций HTTP и URI Рой Филдинг защитил докторскую диссертацию на тему «Архитектурные стили и дизайн архитектуры сетевого программного обеспечения», пятая глава которой была озаглавлена как «Representational State Transfer (REST)»[ref:{"short":"Fielding, R. (2000)","extra":\["Architectural Styles and the Design of Network-based Software Architectures","Representational State Transfer (REST)"\]}](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm).

Как нетрудно убедиться, прочитав эту главу, она представляет собой абстрактный обзор распределённой сетевой архитектуры, вообще не привязанной ни к HTTP, ни к URL. Более того, она вовсе не посвящена правилам дизайна API — в этой главе Филдинг методично _перечисляет ограничения_, с которыми приходится сталкиваться разработчику распределённого сетевого программного обеспечения. Вот они:

- клиент и сервер не знают внутреннего устройства друг друга (клиент-серверная архитектура);
- сессия хранится на клиенте (stateless-дизайн);
- данные должны размечаться как кэшируемые или некэшируемые;
- интерфейсы взаимодействия между компонентами должны быть стандартизированы;
- сетевые системы являются многослойными, т.е. сервер может быть только прокси к другим серверам;
- функциональность клиента может быть расширена через поставку кода с сервера.

На этом определение REST заканчивается. Дальше Филдинг конкретизирует аспекты имплементации систем в указанных ограничениях, но все они точно так же являются совершенно абстрактными. Буквально: «ключевая информационная абстракция в REST — ресурс; любая информация, которой можно дать наименование, может быть ресурсом».

Ключевой вывод, который следует из определения REST по Филдингу-2000, вообще говоря, таков: _любое сетевое ПО в мире соответствует принципам REST_, за очень-очень редкими исключениями.

В самом деле:

- очень сложно представить себе систему, в которой не было бы хоть какой-нибудь стандартизации взаимодействия между компонентами, иначе её просто невозможно будет разрабатывать — в частности, как мы уже отмечали, почти всё сетевое взаимодействие в мире использует стек TCP/IP;
- раз есть интерфейс взаимодействия, значит, под него всегда можно мимикрировать, а значит, требование независимости имплементации клиента и сервера всегда выполнимо;
- раз можно сделать альтернативную имплементацию сервера — значит, можно сделать и многослойную архитектуру, поставив дополнительный прокси между клиентом и сервером;
- поскольку клиент представляет собой вычислительную машину, он всегда хранит хоть какое-то состояние и кэширует хоть какие-то данные;
- наконец, code-on-demand вообще лукавое требование, поскольку в архитектуре фон Неймана<sup><a id="http-api-rest-myth-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-1">1</a></sup> всегда можно объявить данные, полученные по сети, «инструкциями» на некотором формальном языке, а код клиента — их интерпретатором.

Да, конечно, вышеприведённое рассуждение является софизмом, доведением до абсурда. Самое забавное в этом упражнении состоит в том, что мы можем довести его до абсурда и в другую сторону, объявив ограничения REST неисполнимыми. Например, очевидно, что требование code-on-demand противоречит требованию независимости клиента и сервера — клиент должен уметь интерпретировать код с сервера, написанный на вполне конкретном языке. Что касается правила на букву S («stateless»), то систем, в которых сервер _вообще не хранит никакого контекста клиента_ в мире вообще практически нет, поскольку почти ничего полезного для клиента в такой системе сделать нельзя. (Чего, кстати, Филдинг прямым текстом требует: «коммуникация … не может получать никаких преимуществ от того, что на сервере хранится какой-то контекст».)

Наконец, сам Филдинг внёс дополнительную энтропию в вопрос, выпустив в 2008 году разъяснение<sup><a id="http-api-rest-myth-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-2">2</a></sup>, что же он имел в виду. В частности, в этой статье утверждается, что:

- разработка REST API должна фокусироваться на описании медиатипов, представляющих ресурсы; при этом клиент вообще ничего про эти медиатипы знать не должен;
- в REST API не должно быть фиксированных имён ресурсов и операций над ними, клиент должен извлекать эту информацию из ответов сервера.

REST по Филдингу-2008 подразумевает, что клиент, получив каким-то образом ссылку на точку входа в REST API, далее должен быть в состоянии полностью выстроить взаимодействие с API, не обладая вообще никаким априорным знанием о нём, и уж тем более не должен содержать никакого специально написанного кода для работы с этим API. Это требование — гораздо более сильное, нежели принципы, описанные в диссертации 2000 года. В частности, из идеи REST-2008 вытекает отсутствие фиксированных шаблонов URL для выполнения операций над ресурсами — предполагается, что такие URL присутствуют в виде гиперссылок в представлениях ресурсов (эта концепция известна также под названием HATEOAS<sup><a id="http-api-rest-myth-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-3">3</a></sup>). Диссертация же 2000 года никаких строгих определений «гипермедиа», которые препятствовали бы идее конструирования ссылок на основе априорных знаний об API (например, по спецификации), не содержит.

**NB**: оставляя за скобками тот факт, что Филдинг весьма вольно истолковал свою собственную диссертацию, просто отметим, что ни одна существующая система в мире не удовлетворяет описанию REST по Филдингу-2008.

Нам неизвестно, почему из всех обзоров абстрактной сетевой архитектуры именно концепция Филдинга обрела столь широкую популярность; очевидно другое: теория Филдинга, преломившись в умах миллионов программистов (включая самого Филдинга), превратилась в целую инженерную субкультуру. Путём редукции абстракций REST применительно конкретно к протоколу HTTP и стандарту URL родилась химера «RESTful API», конкретного смысла которой никто не знает<sup><a id="http-api-rest-myth-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-4">4</a></sup>.

Хотим ли мы тем самым сказать, что REST является бессмысленной концепцией? Отнюдь нет. Мы только хотели показать, что она допускает чересчур широкую интерпретацию, в чём одновременно кроется и её сила, и её слабость.

С одной стороны, благодаря многообразию интерпретаций, разработчики API выстроили какое-то размытое, но всё-таки полезное представление о «правильной» архитектуре HTTP API. С другой стороны, за отсутствием чётких определений тема REST API превратилась в один из самых больших источников холиваров среди программистов — притом холиваров совершенно бессмысленных, поскольку популярное представление о REST не имеет вообще никакого отношения ни к тому REST, который описан в диссертации Филдинга (и тем более к тому REST, который Филдинг описал в своём манифесте 2008 года).

Термин «архитектурный стиль REST» и производный от него «REST API» в последующих главах мы использовать не будем, поскольку, как видно из написанного выше, в этом нет никакой нужды — на все описанные Филдингом ограничения мы многократно ссылались по ходу предыдущих глав, поскольку, повторимся, распределённое сетевое API попросту невозможно разработать, не руководствуясь ими. Однако HTTP API (подразумевая под этим JSON-over-HTTP эндпойнты, утилизирующие семантику, описанную в стандартах HTTP и URL), как мы его будем определять в последующих главах, фактически соответствует усреднённому представлению о «REST/RESTful API», как его можно найти в многочисленных учебных пособиях.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-1-back) Von Neumann Architecture

    [en.wikipedia.org/wiki/Von_Neumann_architecture](https://en.wikipedia.org/wiki/Von_Neumann_architecture)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-2-back) Fielding, R. T. REST APIs must be hypertext-driven

    [roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven](https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-3-back) HATEOAS

    [en.wikipedia.org/wiki/HATEOAS](https://en.wikipedia.org/wiki/HATEOAS)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-myth-ref-4-back) Gupta, L. What is REST

    [restfulapi.net](https://restfulapi.net/)

### [Глава 35. Составляющие HTTP запросов и их семантика](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics)

Важное подготовительное упражнение, которое мы должны сделать — это дать описание формата HTTP-запросов и ответов и прояснить базовые понятия. Многое из написанного ниже может показаться читателю самоочевидным, но, увы, специфика протокола такова, что даже базовые сведения о нём, без которых мы не сможем двигаться дальше, разбросаны по обширной и фрагментированной документации, и даже опытные разработчики могут не знать тех или иных нюансов. Ниже мы попытаемся дать структурированный обзор протокола в том объёме, который необходим нам для проектирования HTTP API. За более подробным описанием технологии любознательный читатель может обратиться, например, к детальному труду Дэвида Гурли и Брайана Тотти<sup><a id="http-api-requests-semantics-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-1">1</a></sup>.

В описании семантики и формата протокола мы будем руководствоваться свежевышедшим RFC 9110<sup><a id="http-api-requests-semantics-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-2">2</a></sup>, который заменил аж девять предыдущих спецификаций, описывавших разные аспекты технологии (при этом большое количество различной дополнительной функциональности всё ещё покрывается отдельными стандартами. В частности, принципы HTTP-кэширования описаны в отдельном RFC 9111<sup><a id="http-api-requests-semantics-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-3">3</a></sup>, а широко используемый в API метод `PATCH` так и не вошёл в основной RFC и регулируется RFC 5789<sup><a id="http-api-requests-semantics-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-4">4</a></sup>).

HTTP-запрос представляет собой (1) применение определённого глагола к URL с (2) указанием версии протокола, (3) передачей дополнительной мета-информации в заголовках и, возможно, (4) каких-то данных в теле запроса:

```
POST /v1/orders HTTP/1.1
Host: our-api-host.tld
Content-Type: application/json

{
  "coffee_machine_id": 123,
  "currency_code": "MNT",
  "price": "10.23",
  "recipe": "lungo",
  "offer_id": 321,
  "volume": "800ml"
}
```

Ответом на HTTP-запрос будет являться конструкция, состоящая из (1) версии протокола, (2) статус-кода ответа, (3) сообщения, (4) заголовков и, возможно, (5) тела ответа:

```
HTTP/1.1 201 Created
Location: /v1/orders/123
Content-Type: application/json

{
  "id": 123
}
```

**NB**: в HTTP/2 (и будущем HTTP/3) вместо единого текстового формата используются отдельные бинарные фреймы для передачи заголовков и данных[ref:{"short":"Grigorik, I. (2013)","extra":\["High Performance Browser Networking","Chapter 12. HTTP/2"\]}](https://hpbn.co/http2/). Этот факт не влияет на излагаемые архитектурные принципы, но во избежание двусмысленности мы будем давать примеры в формате HTTP/1.1.

##### [1\. URL](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-para-1)

URL — единица адресации в HTTP API (некоторые евангелисты технологии даже используют термин «пространство URL» как синоним для Мировой паутины). Предполагается, что HTTP API должен использовать систему адресов столь же гранулярную, как и предметная область; иными словами, у любых сущностей, которыми мы можем манипулировать независимо, должен быть свой URL.

Формат URL регулируется отдельным стандартом<sup><a id="http-api-requests-semantics-ref-5-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-5">5</a></sup>, который развивает независимое сообщество Web Hypertext Application Technology Working Group (WHATWG). Считается, что концепция URL (вместе с понятием универсального имени ресурса, URN) составляет более общую сущность URI (универсальный идентификатор ресурса). (Разница между URL и URN заключается в том, что URL позволяет _найти_ некоторый ресурс в рамках некоторого протокола доступа, в то время как URN — «внутреннее» имя объекта, которое само по себе никак не помогает получить к нему доступ.)

URL принято раскладывать на составляющие, каждая из которых опциональна. В стандарте перечислены разнообразные исторические наслоения (например, передача логинов и паролей в URL или использование не-UTF кодировки), которые мы опустим. В рамках дизайна HTTP API нам интересны следующие компоненты:

- схема (scheme) — протокол обращения (в нашем случае всегда `https:`);
- хост (host; домен или IP-адрес) — самая крупная единица адресации;
    - домен может включать в себя поддомены;
- порт (port);
- путь (path) — часть URL между именем хоста (с портом) и символами `?`, `#` или концом строки
    - путь принято разбивать по символу `/` и работать с каждой частью как отдельным токеном — но стандарт, вообще говоря, не предписывает никакой семантики такому разбиению;
    - пути с символом `/` и без символа `/` в конце (скажем `/root/leaf` и `/root/leaf/`) с точки зрения стандарта являются разными (и URL, отличающиеся только наличием/отсутствием слэша, считаются разными URL), хотя практически нам неизвестны аргументы в пользу того, чтобы не считать такие пути эквивалентными;
    - пути могут содержать секции `.` и/или `..`, которые предлагается трактовать по аналогии с такими же символами в путях на файловой системе (и, соответственно, считать URL `/root/leaf`, `/root/./leaf`, `/root/branch/../leaf` эквивалентными);
- запрос (query) — часть URL после знака `?` до знака `#` или конца строки;
    - query принято раскладывать на пары `ключ=значение`, разделённые символом `&`; следует вновь иметь в виду, что стандарт не предписывает query строго соответствовать этому формату и не определяет никакой семантики;
    - также стандарт не предписывает никакой нормализации — два URL, которые различаются только порядком ключей в query, по стандарту являются разными URL;
- фрагмент (fragment; также якорь, anchor) — часть URL после знака `#`;
    - фрагмент традиционно рассматривается как адресация внутри запрошенного документа, поэтому многими агентами опускается при выполнении запроса;
    - два URL, отличающихся только значением фрагмента, могут считаться одинаковыми — а могут не считаться, зависит от контекста.

В HTTP-запросах, как правило (но не обязательно) схема, хост и порт опускаются (и считаются совпадающими с параметрами соединения). (Это соглашение, кстати, Филдинг считает самой большой проблемой дизайна протокола.)

Традиционно считается, что части пути описывают строгую иерархию подчинения ресурсов (например, URL конкретной кофе-машины в нашем API мог бы выглядеть как `/places/{id}/coffee-machines/{id}`, поскольку кофе-машина принадлежит строго одной кофейне), а через запрос выражаются нестрогие иерархии и параметры операций (например, URL поиска предложений мог бы выглядеть как `/search?location=<точка на карте>`).

Также стандарт содержит правила сериализации, нормализации и сравнения URL, которые в целом полезно знать разработчику HTTP API.

##### [2\. Заголовки](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-para-2)

Заголовки — это _метаинформация_, привязанная к запросу или ответу. Она может описывать какие-то свойства передаваемых данных (например, `Content-Length`), дополнительные сведения о клиенте или сервере (`User-Agent`, `Date`), или просто содержать поля, не относящиеся непосредственно к смыслу запроса/ответа (например, `Authorization`).

Важное свойство заголовков — это возможность считывать их до того, как получено тело сообщения. Таким образом, заголовки могут, во-первых, сами по себе влиять на обработку запроса или ответа, и ими можно относительно легко манипулировать при проксировании — и многие сетевые агенты действительно это делают, добавляя или модифицируя заголовки по своему усмотрению (в частности, современные веб-браузеры добавляют к запросам целую коллекцию заголовков: `User-Agent`, `Origin`, `Accept-Language`, `Connection`, `Referer`, `Sec-Fetch-*` и так далее, а современное ПО веб-серверов, в свою очередь, автоматически добавляет или модифицирует такие заголовки как `X-Powered-By`, `Date`, `Content-Length`, `Content-Encoding`, `X-Forwarded-For`).

Подобное вольное обращение с заголовками создаёт определённые проблемы, если ваш API предусматривает передачу дополнительных полей метаданных, поскольку придуманные вами имена полей могут случайно совпасть с какими-то из существующих стандартных имён (или ещё хуже — в будущем появится новое стандартное поле, совпадающее с вашим). Долгое время во избежание подобных коллизий использовался префикс `X-`; уже более 10 лет как эта практика объявлена устаревшей и не рекомендуется к использованию (см. подробный разбор вопроса в RFC 6648<sup><a id="http-api-requests-semantics-ref-6-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-6">6</a></sup>), однако отказа от этого префикса по факту не произошло (и многие широко распространённые нестандартные заголовки, например, `X-Forwarded-For`, его всё ещё содержат). Таким образом, использование префикса `X-` вероятность коллизий снижает, но не устраняет. Тот же RFC вполне разумно предлагает использовать вместо `X-` префикс в виде имени компании. (Мы со своей стороны склонны рекомендовать использовать оба префикса в формате `X-ApiName-FieldName`; префикс `X-` для читабельности \[чтобы отличать специальные заголовки от стандартных\], а префикс с именем компании или API — чтобы не произошло коллизий с каким-нибудь другим нестандартным префиксом.)

Помимо прочего заголовки используются как управляющие конструкции — это т.н. «content negotiation», т.е. договорённость клиента и сервера о формате ответа (через заголовки `Accept*`) и условные запросы, позволяющие сэкономить трафик на возврате ответа целиком или частично (через заголовки `If-*`\-заголовки, такие как `If-Range`, `If-Modified-Since` и так далее).

Стандарт предписывает как минимум один обязательный заголовок — `Host`. Ещё несколько заголовков необязательны, но на практике почти всегда используются:

- `Accept`, `Accept-Encoding`, `Content-Type` и `Content-Encoding` для описания форматов данных запроса и ответа;
- `Date` для синхронизации часов клиента и сервера;
- `Content-Length` для описания размера передаваемых данных (некоторые прокси вообще не пропускают запросы без `Content-Length`).

В дальнейших примерах мы эти заголовки будем опускать для лучшей читабельности.

##### [3\. HTTP-глаголы](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-para-3)

Важнейшая составляющая HTTP запроса — это глагол (метод), описывающий операцию, применяемую к ресурсу. RFC 9110 стандартизирует восемь глаголов — `GET`, `POST`, `PUT`, `DELETE`, `HEAD`, `CONNECT`, `OPTIONS` и `TRACE` — из которых нас как разработчиков API интересует первые четыре. `CONNECT`, `OPTIONS` и `TRACE` — технические методы, которые очень редко используются в HTTP API (за исключением `OPTIONS`, который необходимо реализовать, если необходим доступ к API из браузера). Теоретически, `HEAD` (метод получения _только метаданных_, то есть заголовков, ресурса) мог бы быть весьма полезен в HTTP API, но по неизвестным нам причинам практически в этом смысле не используется.

Помимо RFC 9110, множество других RFC предлагают использовать дополнительные HTTP-глаголы (такие, например, как `COPY`, `LOCK`, `SEARCH` — полный список можно найти в реестре<sup><a id="http-api-requests-semantics-ref-7-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-7">7</a></sup>), однако из всего разнообразия предложенных стандартов лишь один имеет широкое хождение — метод `PATCH`. Причины такого положения дел довольно тривиальны — этих пяти методов (`GET`, `POST`, `PUT`, `DELETE`, `PATCH`) достаточно для почти любого HTTP API.

HTTP-глагол определяет два важных свойства HTTP-вызова:

- его семантику (что представляет собой операция);
- его побочные действия, а именно:
    - является ли запрос модифицирующим (и можно ли кэшировать ответ);
    - является ли запрос идемпотентным.

| Глагол | Семантика                                                                                | Безопасный (немодифицирующий) | Идемпотентный | Может иметь тело |
| ------ | ---------------------------------------------------------------------------------------- | ----------------------------- | ------------- | ---------------- |
| GET    | Возвращает представление ресурса                                                         | да                            | да            | не рекомендуется |
| PUT    | Заменяет (полностью перезаписывает) ресурс согласно данным, переданным в теле запроса    | нет                           | да            | да               |
| DELETE | Удаляет ресурс                                                                           | нет                           | да            | не рекомендуется |
| POST   | Обрабатывает запрос в соответствии со своим внутренним устройством                       | нет                           | нет           | да               |
| PATCH  | Модифицирует (частично перезаписывает) ресурс согласно данным, переданным в теле запроса | нет                           | нет           | да               |

**NB**: распространено мнение, что метод `POST` предназначен только для создания новых ресурсов. Это совершенно не так, создание ресурса только один из вариантов «обработки запроса согласно внутреннему устройству» эндпойнта.

Важное свойство модифицирующих идемпотентных глаголов — это то, что **URL запроса является его ключом идемпотентности**. `PUT /url` полностью перезаписывает ресурс, заданный своим URL (`/url`), и, таким образом, повтор запроса не изменяет ресурс. Аналогично, повторный вызов `DELETE /url` должен оставить систему в том же состоянии (ресурс `/url` удалён). Учитывая, что метод `GET /url` семантически должен вернуть представление целевого ресурса `/url`, то, если этот метод реализован, он должен возвращать консистентное предыдущим `PUT` / `DELETE` представление. Если ресурс был перезаписан через `PUT /url`, `GET /url` должен вернуть представление, соответствующее переданному в `PUT /url` телу (в случае JSON-over-HTTP API это, как правило, просто означает, что `GET /url` возвращает в точности тот же контент, что был передан в `PUT /url`, с точностью до значений полей по умолчанию). `DELETE /url` обязан удалить указанный ресурс — так, что `GET /url` должен вернуть `404` или `410`.

Идемпотентность и симметричность методов `GET` / `PUT` / `DELETE` влечёт за собой нежелательность для `GET` и `DELETE` запросов иметь тело (поскольку этому телу невозможно приписать никакой осмысленной роли). Однако (по-видимому в связи с тем, что многие разработчики попросту не знают семантику этих методов) распространённое ПО веб-серверов обычно разрешает этим методам иметь тело запроса и транслирует его дальше к коду обработки эндпойнта (использование этой практики мы решительно не рекомендуем).

Достаточно очевидным образом ответы на модифицирующие запросы не кэшируются (хотя при определённых условиях закэшированный ответ метода `POST` может быть использован при последующем `GET`\-запросе) и, таким образом, повторный `POST` / `PUT` / `DELETE` / `PATCH` запрос обязательно будет доставлен до конечного сервера (ни один промежуточный агент не имеет права ответить из кэша). В случае `GET`\-запроса это, вообще говоря, неверно — гарантией может служить только наличие в ответе директив кэширования `no-store` или `no-cache`.

Один из самых частых антипаттернов разработки HTTP API — это использование HTTP-глаголов в нарушение их семантики:

- Размещение модифицирующих операций за `GET`:
    - промежуточные агенты могут ответить на такой запрос из кэша, если какая-то из директив кэширования отсутствует, либо, напротив, повторить запрос при получении сетевого таймаута;
    - некоторые агенты считают себя вправе переходить по таким ссылкам без явного волеизъявления пользователя или разработчика; например, социальные сети и мессенджеры выполняют такие вызовы для генерации оформления ссылки, если пользователь пытается ей поделиться.
- Размещение неидемпотентных операций за идемпотентными методами `PUT` / `DELETE`. Хотя промежуточные агенты редко автоматически повторяют модифицирующие запросы, тем не менее это легко может сделать используемый разработчиком клиента или сервера фреймворк. Обычно эта ошибка сочетается с наличием у `DELETE`\-запроса тела (чтобы всё-таки отличать, что конкретно нужно перезаписать или удалить), что является само по себе проблемой, так как любой сетевой агент вправе это тело проигнорировать.
- Несоблюдение требования симметричности операций `GET` / `PUT` / `DELETE`:
    - например, после выполнения `DELETE /url` операция `GET /url` продолжает возвращать какие-то данные или `PUT /url` ориентируется не на URL, а на данные внутри тела запроса для определения сущности, над которой выполняется операция, и, таким образом, `GET /url` никак не может вернуть представление объекта, только что переданного в `PUT /url`.

##### [4\. Статус-коды](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-para-4)

Статус-код — это машиночитаемое описание результата HTTP-запроса в виде трёхзначного числа. Все статус-коды делятся на пять больших групп:

- `1xx` — информационные (фактически, какое-то хождение имеет разве что `100 Continue`);
- `2xx` — коды успеха операции;
- `3xx` — коды перенаправлений (индицируют необходимость выполнения дополнительных действий, чтобы считать операцию успешной);
- `4xx` — клиентские ошибки;
- `5xx` — серверные ошибки.

**NB**: разделение на группы по первой цифре кода имеет очень важное практическое значение. В случае, если возвращаемый сервером код ошибки `xyz` неизвестен клиенту, согласно спецификации клиент обязан выполнить то действие, которое выполнил бы при получении ошибки `x00`.

В основе технологии статус-кодов лежит понятное желание сделать ошибки машиночитаемыми, так, чтобы все промежуточные агенты могли понять, что конкретно произошло с запросом. Номенклатура статус-кодов HTTP действительно подробно описывает почти любые проблемы, которые могут случиться с HTTP-запросом: недопустимые значения `Accept-*`\-заголовков, отсутствующий `Content-Length`, неподдерживаемый HTTP-метод, слишком длинный URI и так далее.

К сожалению, для описаний ошибок, возникающих в бизнес-логике, номенклатура статус-кодов HTTP совершенно недостаточна и вынуждает использовать статус-коды в нарушение стандарта и/или обогащать ответ дополнительной информацией об ошибке. Проблемы имплементации системы ошибок в HTTP API мы обсудим подробнее в главе «Работа с ошибками в HTTP API».

**NB**: обратите внимание на проблему дизайна спецификации. По умолчанию все `4xx` коды не кэшируются, за исключением: `404`, `405`, `410`, `414`. Мы не сомневаемся, что это было сделано из благих намерений, но подозреваем, что множество людей, знающих об этих тонкостях, примерно совпадает с множеством редакторов спецификации HTTP.

#### Важное замечание о кэшировании

Кэширование — исключительно важная часть любой современной микросервисной архитектуры, и велик соблазн управлять им на уровне протокола — благо, стандарт предоставляет весьма функциональную и продвинутую функциональность работы с кэшами. Однако автор этой книги должен предостеречь читателя: если вы планируете имплементировать такую логику, прочитайте стандарт очень внимательно. Неверное толкование тех или иных параметров кэширования может приводить к крайне неприятным ситуациям; например, в практике автора был случай, когда случайное удаление настроек для определённой географической области (эндпойнт начал возвращать `404`) привело к неработоспособности сервиса на протяжении нескольких часов, поскольку разработчики протокола не учли, что статус `404` по умолчанию кэшируется, и клиенты просто не запрашивают новую версию настроек, пока не истечёт время жизни кэша.

#### Важное замечание о консистентности

Один и тот же параметр в разных ситуациях может находиться в разных частях запроса. Скажем, идентификатор партнёра, совершающего запрос, может быть передан:

- в имени поддомена `{partner_id}.domain.tld`;
- как часть пути `/v1/{partner_id}/orders`;
- как query-параметр `/v1/orders?partner_id=<partner_id>`;
- как заголовок

    ```
    GET /v1/orders HTTP/1.1
    X-ApiName-Partner-Id: <partner_id>
    ```

- как поле в теле запроса

    ```
    POST /v1/orders/retrieve HTTP/1.1

    {
      "partner_id": <partner_id>
    }
    ```

Возможны и более экзотические варианты: размещение параметра в схеме запроса или заголовке `Content-Type`.

Однако при перемещении параметра между различными составляющими запроса мы столкнёмся с тремя неприятными явлениями:

- некоторые значения чувствительны к регистру (путь, query-параметры, имена полей в JSON), некоторые нет (домен, имена заголовков);
    - при этом со _значениями_ заголовков и вовсе неразбериха: часть из них по стандарту обязательно нечувствительна к регистру (в частности, `Content-Type`), а часть, напротив, обязательно чувствительна (например, `ETag`);

- наборы допустимых символов и правила экранирования также различны для разных частей запроса
    - для path, например, стандарта экранирования символов `/`, `?` и `#` не существует;
    - символы unicode могут использоваться в доменных именах (хотя эта функциональность не везде поддерживается) только через своеобразную технику кодирования под названием Punycode<sup><a id="http-api-requests-semantics-ref-8-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-8">8</a></sup>;

- для разных частей запросов используется разный кейсинг:
    - `kebab-case` для домена, заголовков и пути;
    - `snake_case` для query-параметров;
    - `snake_case` или `camelCase` для тела запроса;

    При этом использование и `snake_case`, и `camelCase` в доменном имени невозможно, так как знак подчеркивания в доменных именах недопустим, а заглавные буквы будут приведены к строчным.

Чисто теоретически возможно использование `kebab-case` во всех случаях, но в большинстве языков программирования имена переменных и полей объектов в `kebab-case` недопустимы, что приведёт к неудобству работы с таким API.

Короче говоря, ситуация с кейсингом настолько плоха и запутана, что консистентного и удобного решения попросту нет. В этой книге мы придерживаемся следующего правила: токены даются в том кейсинге, который является общепринятым для той секции запроса, в которой находится токен; если положение токена меняется, то меняется и кейсинг. (Мы далеки от того, чтобы рекомендовать этот подход всюду; наша общая рекомендация, скорее — не умножать энтропию и пытаться минимизировать такого рода коллизии.)

**NB**: вообще говоря, JSON исходно — это JavaScript Object Notation, а в языке JavaScript кейсинг по умолчанию - `camelCase`. Мы, тем не менее, позволим себе утверждать, что JSON давно перестал быть форматом данных, привязанным к JavaScript, и в настоящее время используется для организации взаимодействия агентов, реализованных на любых языках программирования. Использование `snake_case`, по крайней мере, позволяет легко перебрасывать параметр из query в тело и обратно, что, обычно, является наиболее частотным кейсом при разработке HTTP API. Впрочем, обратный вариант (использование `camelCase` в именах query-параметров) тоже допустим.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-1-back) [Gourley D., Totty, B. (2002)](https://twirl.github.io/The-API-Book/API.ru.html#bibliography-gourley-totty-http)
- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-2-back) RFC 9110 HTTP Semantics

    [www.rfc-editor.org/rfc/rfc9110.html](https://www.rfc-editor.org/rfc/rfc9110.html)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-3-back) RFC 9111 HTTP Caching

    [www.rfc-editor.org/rfc/rfc9111.html](https://www.rfc-editor.org/rfc/rfc9111.html)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-4-back) PATCH Method for HTTP

    [www.rfc-editor.org/rfc/rfc5789.html](https://www.rfc-editor.org/rfc/rfc5789.html)

- [<sup>5</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-5-back) URL Living Standard

    [url.spec.whatwg.org](https://url.spec.whatwg.org/)

- [<sup>6</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-6-back) Deprecating the "X-" Prefix and Similar Constructs in Application Protocols

    [www.rfc-editor.org/rfc/rfc6648](https://www.rfc-editor.org/rfc/rfc6648)

- [<sup>7</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-7-back) Hypertext Transfer Protocol (HTTP) Method Registry

    [www.iana.org/assignments/http-methods/http-methods.xhtml](https://www.iana.org/assignments/http-methods/http-methods.xhtml)

- [<sup>8</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-requests-semantics-ref-8-back) Punycode: A Bootstring encoding of Unicode for Internationalized Domain Names in Applications (IDNA)

    [www.rfc-editor.org/rfc/rfc3492.txt](https://www.rfc-editor.org/rfc/rfc3492.txt)

### [Глава 36. Организация HTTP API согласно принципам REST](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-organizing)

Перейдём теперь к конкретике: что конкретно означает «следовать семантике протокола» и «разрабатывать приложение в соответствии с архитектурным стилем REST». Напомним, речь идёт о следующих принципах:

- операции должны быть stateless;
- данные должны размечаться как кэшируемые или некэшируемые;
- интерфейсы взаимодействия между компонентами должны быть стандартизированы;
- сетевые системы многослойны.

Эти принципы мы должны применить к протоколу HTTP, соблюдая дух и букву стандарта:

- URL операции должен идентифицировать ресурс, к которому применяется действие, и быть ключом кэширования для `GET` и ключом идемпотентности — для `PUT` и `DELETE`;
- HTTP-глаголы должны использоваться в соответствии с их семантикой;
- свойства операции (безопасность, кэшируемость, идемпотентность, а также симметрия `GET` / `PUT` / `DELETE`\-методов), заголовки запросов и ответов, статус-коды ответов должны соответствовать спецификации.

**NB**: мы намеренно опускаем многие тонкости стандарта:

- ключ кэширования фактически является составным \[включает в себя заголовки запроса\], если в ответе содержится заголовок `Vary`;
- ключ идемпотентности также может быть составным, если в запросе содержится заголовок `Range`;
- политика кэширования в отсутствие явных заголовков кэширования определяется не только глаголом, но и статус-кодом и другими заголовками запроса и ответа, а также политиками платформы;

    — в целях сохранения размеров глав в рамках разумного касаться этих вопросов мы не будем, но стандарт всё-таки рекомендуем внимательно прочитать.

Рассмотрим построение HTTP API на конкретном примере. Представим себе, например, процедуру старта приложения. Как правило, на старте требуется, используя сохранённый токен аутентификации, получить профиль текущего пользователя и важную информацию о нём (в нашем случае — текущие заказы). Мы можем достаточно очевидным образом предложить для этого эндпойнт:

```
GET /v1/state HTTP/1.1
Authorization: Bearer <token>
→
HTTP/1.1 200 OK

{ "profile", "orders" }
```

Получив такой запрос, сервер проверит валидность токена, получит идентификатор пользователя `user_id`, обратится к базе данных и вернёт профиль пользователя и список его заказов.

Подобный простой монолитный API-сервис нарушает сразу несколько архитектурных принципов REST:

- нет очевидного способа кэшировать ответ на клиенте (данные о заказе часто меняются и их нет смысла сохранять);
- операция является stateful, т.к. сервер должен хранить токены в памяти, чтобы извлечь из них идентификатор клиента (к которому привязаны запрошенные данные);
- система однослойна (и таким образом вопрос об унифицированном интерфейсе бессмыслен).

Пока вопросы масштабирования бэкенда нас не волнуют, подобная схема прекрасно работает. Однако, с ростом количества пользователей и функциональности сервиса (а также количества программистов, над ним работающим), мы рано или поздно столкнёмся с тем, что подобная монолитная архитектура нам слишком дорого обходится. Допустим, мы приняли решение декомпозировать единый бэкенд на четыре микросервиса:

- сервис A, проверяющий авторизационные токены;
- сервис B, хранящий профили пользователей;
- сервис C, хранящий заказы пользователей;
- сервис-гейтвей D, который маршрутизирует запросы между другими микросервисами.

Таким образом, запрос будет проходить по следующему пути:

- гейтвей D получит запрос и отправит его в сервисы B и C;
- сервисы B и C обратятся к сервису A, проверят токен (переданный через проксирование заголовка `Authorization` или как явный параметр запроса), и вернут данные по запросу — профиль пользователя и список его заказов;
- сервис D скомбинирует ответы сервисов B и C и вернёт их клиенту.

Нетрудно заметить, что мы тем самым создаём излишнюю нагрузку на сервис A: теперь к нему обращается каждый из вложенных микросервисов; даже если мы откажемся от аутентификации пользователей в конечных сервисах, оставив её только в сервисе D, проблему это не решит, поскольку сервисы B и C самостоятельно выяснить идентификатор пользователя не могут. Очевидный способ избавиться от лишних запросов — сделать так, чтобы однажды полученный `user_id` передавался остальным сервисам по цепочке:

- гейтвей D получает запрос и через сервис A меняет токен на `user_id`
- гейтвей D обращается к сервису B

    ```
    GET /v1/profiles/{user_id}
    ```

    и к сервису C

    ```
    GET /v1/orders?user_id=<user id>
    ```

**NB**: мы использовали нотацию `/v1/orders?user_id`, а не, допустим, `/v1/users/{user_id}/orders` по двум причинам:

- сервис текущих заказов хранит заказы, а не пользователей — логично если URL будет это отражать;
- если нам потребуется в будущем позволить нескольким пользователям делать общий заказ, нотация `/v1/orders?user_id` будет лучше отражать отношения между сущностями.

    Более подробно о принципах формирования URL в HTTP API мы поговорим в следующей главе.

Теперь сервисы B и C получают запрос в таком виде, что им не требуется выполнение дополнительных действий (идентификации пользователя через сервис А) для получения результата. Тем самым мы переформулировали запрос так, что он _не требует от (микро)сервиса обращаться за данными за пределами его области ответственности_, добившись соответствия stateless-принципу.

Отметим, что вопрос о разнице между **stateless** и **stateful** подходами, вообще говоря, не имеет простого ответа. Микросервис B сам по себе хранит состояние клиента (профиль пользователя) и, таким образом, является stateful с точки зрения буквы диссертации Филдинга. Тем не менее, мы скорее интуитивно соглашаемся с тем, что хранить данные по профилю пользователя и только проверять валидность токена — это более правильный подход, чем хранить те же данные плюс кэш токенов, из которого можно извлечь идентификатор пользователя. Фактически, мы говорим здесь о _логическом_ принципе разделения уровней абстракции, который мы подробно обсуждали в [соответствующей главе](https://twirl.github.io/The-API-Book/API.ru.html#api-design-separating-abstractions):

- **микросервисы разрабатываются так, чтобы иметь чётко очерченную зону ответственности и не хранить данные, относящиеся к другим уровням абстракции**;
- такие «внешние» данные являются лишь идентификаторами контекстов, и сам микросервис никак их не трактует;
- если всё же какие-то дополнительные операции с внешними данными требуется производить (например, проверять, авторизована ли запрашивающая сторона на выполнение операции), то следует **организовать операцию так, чтобы свести её к проверке целостности переданных данных**.

    В нашем примере мы могли бы избавиться от лишних запросов к сервису A иначе — начав использовать stateless-токены, например, по стандарту JWT<sup><a id="http-api-rest-organizing-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-organizing-ref-1">1</a></sup>. Тогда сервисы B и C смогут сами раскодировать токен и извлечь идентификатор пользователя.

Пойдём теперь чуть дальше и подметим, что профиль пользователя меняется достаточно редко, и нет никакой нужды каждый раз получать его заново — мы могли бы организовать кэш профилей на стороне гейтвея D. Для этого нам нужно сформировать ключ кэша, которым фактически является идентификатор клиента. Мы можем пойти длинным путём:

- перед обращением в сервис B составить ключ и обратиться к кэшу;
- если данные имеются в кэше, ответить клиенту из кэша; иначе обратиться к сервису B и сохранить полученные данные в кэш.

А можем просто положиться на HTTP-кэширование, которое наверняка или реализовано в нашем фреймворке, или добавляется в качестве плагина за пять минут. Тогда гейтвей D обратится к ресурсу `/v1/profiles/{user_id}` в сервисе B, получит данные и заголовки с параметрами кэширования, и сохранит их локально.

Теперь рассмотрим сервис C. Результат его работы мы тоже могли бы кэшировать, однако состояние текущего заказа меняется гораздо чаще профиля пользователя, и возврат неверного состояния может приводить к крайне неприятным последствиям. Вспомним, однако, описанный нами в главе «[Стратегии синхронизации](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies)» паттерн оптимистичного управления параллелизмом: для корректной работы сервиса нам нужна ревизия состояния ресурса, и ничто не мешает нам воспользоваться этой ревизией как ключом кэша. Пусть сервис С возвращает нам тэг, соответствующий текущему состоянию заказов пользователя:

```
GET /v1/orders?user_id=<user_id> HTTP/1.1
→
HTTP/1.1 200 OK
ETag: <ревизия>
…
```

И тогда гейтвей D при выполнении запроса может:

1.  Закэшировать результат выполнения `GET /v1/orders?user_id=<user_id>`, использовав URL как ключ кэша
2.  При получении повторного запроса:
    - найти закэшированное состояние, если оно есть;
    - отправить запрос к сервису C вида

        ```
        GET /v1/orders?user_id=<user_id> HTTP/1.1
        If-None-Match: <ревизия>
        ```

    - если сервис C отвечает статусом `304 Not Modified`, вернуть данные из кэша;
    - если сервис C отвечает новой версией данных, сохранить её в кэш и вернуть обновленный результат клиенту.

Использовав такое решение \[функциональность управления кэшом через `ETag` ресурсов\], мы автоматически получаем ещё один приятный бонус: эти же данные пригодятся нам, если пользователь попытается создать новый заказ. Если мы используем оптимистичное управление параллелизмом, то клиент должен передать в запросе актуальную ревизию ресурса `orders`:

```
POST /v1/orders HTTP/1.1
If-Match: <ревизия>
```

Гейтвей D подставляет в запрос идентификатор пользователя и формирует запрос к сервису C:

```
POST /v1/orders?user_id=<user_id> HTTP/1.1
If-Match: <ревизия>
```

Если ревизия правильная, гейтвей D может сразу же получить в ответе сервиса C обновлённый список заказов и его ревизию:

```
HTTP/1.1 201 Created
Content-Location: /v1/orders?user_id=<user_id>
ETag: <новая ревизия>

{ /* обновлённый список текущих заказов */ }
```

и обновить кэш в соответствии с новыми данными.

**Важно**: обратите внимание на то, что, после всех преобразований, мы получили систему, в которой мы можем _убрать гейтвей D_ и возложить его функции непосредственно на клиентский код. В самом деле, ничто не мешает клиенту:

- хранить на своей стороне `user_id` (либо извлекать его из токена, если формат позволяет) и последний полученный `ETag` состояния списка заказов;
- вместо одного запроса `GET /v1/state` сделать два запроса (`GET /v1/profiles/{user_id}` и `GET /v1/orders?user_id=<user_id>`), благо протокол HTTP/2 поддерживает мультиплексирование запросов по одному соединению;
- поддерживать на своей стороне кэширование результатов обоих запросов с помощью стандартных библиотек и/или плагинов.

С точки зрения реализации сервисов B и C наличие или отсутствие гейтвея перед ними ни на что не влияет кроме механики авторизации запросов. Мы также можем добавить и второй гейтвей в цепочку, если, скажем, мы захотим разделить хранение заказов на «горячее» и «холодное» хранилища, или заставить какой-то из сервисов B или C работать в качестве гейтвея.

Если мы теперь обратимся к началу главы, мы обнаружим, что мы построили систему, полностью соответствующую требованиям REST:

- запросы к сервисам уже несут в себе все данные, которые необходимы для выполнения запроса;
- интерфейс взаимодействия настолько унифицирован, что мы можем передавать функции гейтвея клиенту или другому промежуточному агенту;
- политика кэширования каждого вида данных размечена.

Повторимся, что мы можем добиться того же самого, использовав RPC-протоколы или разработав свой формат описания статуса операции, параметров кэширования, версионирования ресурсов, приписывания и чтения метаданных и параметров операции. Но автор этой книги позволит себе, во-первых, высказать некоторые сомнения в качестве получившегося решения, и, во-вторых, отметить значительное количество кода, которое придётся написать для реализации всего вышеперечисленного.

#### Авторизация stateless-запросов

Рассмотрим подробнее подход, в котором авторизационного сервиса A фактически нет (точнее, он имплементируется как библиотека или локальный демон в составе сервисов B, C и D), и все необходимые данные зашифрованы в самом токене авторизации. Тогда каждый сервис должен выполнять следующие действия:

1.  Получить запрос вида

    ```
    GET /v1/profiles/{user_id}
    Authorization: Bearer <token>
    ```

2.  Расшифровать токен и получить вложенные данные, например, в следующем виде:

    ```
    {
      // Идентификатор пользователя-
      // владельца токена
      "user_id",
      // Таймстемп создания токена
      "iat"
    }
    ```

3.  Проверить, что указанные в данных токена права доступа соответствуют параметрам операции — в данном случае сравнить `user_id`, переданный как query-параметр, и `user_id`, содержащийся в токене — и вынести решение о (не)допустимости операции.

Требование передавать `user_id` дважды и потом сравнивать две копии друг с другом может показаться нелогичным и избыточным. Однако это мнение ошибочно, и проистекает из широко распространённого (анти)паттерна, с описания которого мы начали главу, а именно — stateful-определение параметров операции:

```
GET /v1/profile
Authorization: Bearer <token>
```

Такой эндпойнт фактически выполняет все три операции контроля доступа:

- _аутентифицирует_ пользователя путём поиска токена в кэше токенов;
- _идентифицирует_ пользователя путём извлечения связанного с токеном идентификатора;
- _авторизует_ операцию, дополнив её параметры и _неявно_ предполагая, что пользователь всегда имеет доступ к своим собственным данным.

Проблема с таким подходом заключается в том, что _разделить_ эти операции не представляется возможным. Вспомним описанные нами в главе «[Аутентификация партнёров и авторизация вызовов API](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-aa)» варианты авторизации вызовов API: в любой достаточно сложной системе нам придётся разрешать пользователю X выполнять действия от имени пользователя Y — например, если мы продаем функциональность заказа кофе как B2B API, и директор компании-партнёра желает лично или программно контролировать заказы, сделанные сотрудниками компании.

В случае «тройственного» эндпойнта проверки доступа мы можем только разработать новый эндпойнт с новым интерфейсом. В случае stateless-токенов мы можем поступить так:

1.  Зашифровать в токене _список_ пользователей, доступ к которым возможен через предъявление настоящего токена:

    ```
    {
      // Идентификаторы пользователей,
      // доступ к профилям которых
      // разрешён с настоящим токеном
      "user_ids",
      // Таймстемп создания токена
      "iat"
    }
    ```

2.  Изменить проверку авторизации (=внести изменения в код локального SDK или демона) так, чтобы она разрешала выполнение операции, если `user_id` в query-параметре содержится в списке `user_ids` токена.

    Этот подход можно в дальнейшем усложнять: добавлять гранулярные разрешения выполнять конкретные операции, вводить уровни доступа, проверку прав в реальном времени через дополнительный вызов ACL-сервиса и так далее.

Важно, что кажущаяся избыточность перестала быть таковой: `user_id` в запросе теперь не дублируется в данных токена; эти идентификаторы имеют разный смысл: _над каким ресурсом_ исполняется операция и _кто_ исполняет операцию. Совпадение этих двух сущностей — пусть частотный, но всё же частный случай. Что, к сожалению, не отменяет его неочевидности и возможности легко забыть выполнить проверку в коде. Таков путь.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-rest-organizing-ref-1-back) JSON Web Token (JWT)

    [www.rfc-editor.org/rfc/rfc7519](https://www.rfc-editor.org/rfc/rfc7519)

### [Глава 37. Разработка номенклатуры URL ресурсов. CRUD-операции](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud)

Как мы уже отмечали в предыдущих главах, стандарты HTTP и URL, а также принципы REST, не предписывают определённой семантики значимым компонентам URL (в частности, частям path и парам ключ-значение в query). **Правила организации URL в HTTP API существуют _только_ для читабельности кода и удобства разработчика**. Что, впрочем, совершенно не означает, что они неважны: напротив, URL в HTTP API являются средством выразить уровни абстракции и области ответственности объектов. Правильный дизайн иерархии сущностей в API должен быть отражён в правильном дизайне номенклатуры URL.

**NB**: отсутствие строгих правил естественным образом привело к тому, что многие разработчики их просто придумали сами для себя. Некоторые наиболее распространённые стихийные практики, например, требование использовать в URL только существительные, в советах по разработке HTTP API в Интернете часто выдаются за стандарты или требования REST, которыми они не являются. Тем не менее, демонстративное игнорирование таких самопровозглашённых правил тоже не лучший подход для провайдера API, поскольку он увеличивает шансы быть неверно понятым.

Традиционно частям URL приписывается следующая семантика:

- части path (фрагменты пути между символами `/`) используются для организации вложенных сущностей вида `/partner/{id}/coffee-machines/{id}`; при этом путь часто может наращиваться, т.е. к конкретному пути продолжают приписываться новые суффиксы, указывающие на подчинённые ресурсы;
- query используется для организации нестрогой иерархии (отношений «многие ко многим», например `/recipes/?partner=<partner_id>`) либо как способ передать параметры операции (`/search/?recipe=lungo`).

Подобная конвенция достаточно хорошо подходит для того, чтобы отразить номенклатуру сущностей почти любого API, поэтому следовать ей вполне разумно (и, наоборот, демонстративное нарушение этого устоявшегося соглашения чревато тем, что разработчики вас просто неправильно поймут). Однако подобная некодифицированная и размытая концепция неизбежно вызывает множество разночтений в конкретных моментах:

1.  Где заканчиваются метаданные операции и начинаются «просто» данные и насколько допустимо дублировать поля и там, и там? Например, одна из распространённых практик в HTTP API — индицировать тип возвращаемых данных путём добавки «расширения» к URL, подобно именам файлов в файловых системах (т.е. при обращении к ресурсу `/v1/orders/{id}.xml` ответ будет получен в формате XML, а при обращении к `/v1/orders/{id}.json` — в формате JSON). С одной стороны, для определения формата данных предназначены `Accept*`\-заголовки. С другой стороны, читабельность кода явно повышается с введением параметра «формат» прямо в URL.
2.  Как следствие предыдущего пункта — каким образом в HTTP API правильно указывать версию самого API? Навскидку можно предложить как минимум три варианта, каждый из которых вполне соответствует букве стандарта:
    - как path-параметр: `/v1/orders/{id}`;
    - как query-параметр: `/orders/{id}?version=1`;
    - как заголовок:

        ```
        GET /orders/{id} HTTP/1.1
        X-OurCoffeeAPI-Version: 1
        ```

    Сюда можно приплюсовать и более экзотические варианты, такие как указание схемы в кастомизированном медиатипе или протоколе запроса.

3.  Каким образом организовывать эндпойнты, связывающие две сущности, между которыми нет явных отношений подчинения? Скажем, каким должен быть URL запуска приготовления лунго на конкретной кофе-машине?
    - `/coffee-machines/{id}/recipes/lungo/prepare`
    - `/recipes/lungo/coffee-machines/{id}/prepare`
    - `/coffee-machines/{id}/prepare?recipe=lungo`
    - `/recipes/lungo/prepare?coffee_machine_id=<id>`
    - `/prepare?coffee_machine_id=<id>&recipe=lungo`
    - `/?action=prepare&coffee_machine_id=<id>&recipe=lungo`

    Все эти варианты семантически вполне допустимы и в общем-то равноправны.

4.  Насколько строго должна выдерживаться буквальная интерпретация конструкции `ГЛАГОЛ /ресурс`? Если мы принимаем правило «части URL обязаны быть существительными» (и ведь странно применять глагол к глаголу!), то в примерах выше должно быть не `prepare`, а `preparator` или `preparer` (а вариант `/action=prepare&coffee_machine_id=<id>&recipe=lungo` вовсе недопустим, так как нет объекта действия), что, честно говоря, лишь добавляет визуального шума в виде суффиксов «ator», но никак не способствует большей лаконичности и однозначности понимания.
5.  Если сигнатура вызова по умолчанию модифицирующая или неидемпотентная, означает ли это, что операция _обязана_ быть модифицирующей / неидемпотентной? Двойственность смысловой нагрузки глаголов (семантика vs побочные действия) порождает неопределённость в вопросах организации API. Рассмотрим, например, ресурс `/v1/search`, осуществляющий поиск предложений кофе в нашем учебном API. С каким глаголом мы должны к нему обращаться?
    - С одной стороны, `GET /v1/search?query=<поисковый запрос>` позволяет явно продекларировать, что никаких посторонних эффектов у этого запроса нет (никакие данные не перезаписываются) и результаты его можно кэшировать (при условии, что все значимые параметры передаются в URL).
    - С другой стороны, согласно семантике операции, `GET /v1/search` должен возвращать _представление ресурса `search`_. Но разве результаты поиска являются представлением ресурса-поисковика? Смысл операции «поиск» гораздо точнее описывается фразой «обработка запроса в соответствии с внутренней семантикой ресурса», т.е. соответствует методу `POST`. Кроме того, можем ли мы вообще говорить о кэшировании поисковых запросов? Страница результатов поиска формируется динамически из множества источников, и повторный запрос с той же поисковой фразой почти наверняка выдаст другой список результатов.

    Иными словами, для любых операций, результат которых представляет собой результат работы какого-то алгоритма (например, список релевантных предложений по запросу) мы всегда будем сталкиваться с выбором, что важнее: семантика глагола или отсутствие побочных эффектов? Кэширование ответа или индикация того, что операция вычисляет результаты на лету?

    **NB**: эта дихотомия волнует не только нас, но и авторов стандарта, которые в конечном итоге предложили новый глагол `QUERY`<sup><a id="http-api-urls-crud-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-ref-1">1</a></sup>, который по сути является немодифицирующим `POST`. Мы, однако, сомневаемся, что он получит широкое распространение — поскольку уже существующий `SEARCH`<sup><a id="http-api-urls-crud-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-ref-2">2</a></sup> оказался в этом качестве никому не нужен.

Простых ответов на вопросы выше у нас, к сожалению, нет. В рамках настоящей книги мы придерживаемся следующего подхода: сигнатура вызова в первую очередь должна быть лаконична и читабельна. Усложнение сигнатур в угоду абстрактным концепциям нежелательно. Применительно к указанным проблемам это означает, что:

1.  Метаданные операции не должны менять смысл операции; если запрос доходит до конечного микросервиса вообще без заголовков, он всё ещё должен быть выполним, хотя какая-то вспомогательная функциональность может деградировать или отсутствовать.
2.  Мы используем указание версии в path по одной простой причине: все остальные способы сделать это имеют смысл, если и только если при изменении мажорной версии протокола номенклатура URL останется прежней. Но, если номенклатура ресурсов может быть сохранена, то нет никакой нужды нарушать обратную совместимость.
3.  Иерархия ресурсов выдерживается там, где она однозначна (т.е., если сущность низшего уровня абстракции однозначно подчинена сущности высшего уровня абстракции, то отношения между ними будут выражены в виде вложенных путей).
    - Если есть сомнения в том, что иерархия в ходе дальнейшего развития API останется неизменной, лучше завести новый верхнеуровневый префикс, а не вкладывать новые сущности в уже существующие.
4.  Для выполнения «кросс-доменных» операций (т.е. при необходимости сослаться на объекты разных уровней абстракции в одном вызове) предпочтительнее завести специальный ресурс, выполняющий операцию (т.е. в примере с кофе-машинами и рецептами автор этой книги выбрал бы вариант `/prepare?coffee_machine_id=<id>&recipe=lungo`).
5.  Семантика HTTP-вызова приоритетнее ложного предупреждения о небезопасности/неидемпотентности (в частности, если операция является безопасной, но ресурсозатратной, с нашей точки зрения вполне разумно использовать метод `POST` для индикации этого факта).

**NB**: отметим, что передача параметров в виде пути или query-параметра в URL влияет не только на читабельность. Вернёмся к примеру из предыдущей главы и представим, что гейтвей D реализован в виде stateless прокси с декларативной конфигурацией. Тогда получать от клиента запрос в виде:

- `GET /v1/state?user_id=<user_id>`

    и преобразовывать в пару вложенных запросов

- `GET /v1/profiles?user_id=<user_id>`
- `GET /v1/orders?user_id=<user_id>`

    гораздо удобнее, чем извлекать идентификатор из path и преобразовывать его в query-параметр. Первую операцию \[замена одного path целиком на другой\] достаточно просто описать декларативно, и в большинстве ПО для веб-серверов она поддерживается из коробки. Напротив, извлечение данных из разных компонентов и полная пересборка запроса — достаточно сложная функциональность, которая, скорее всего, потребует от гейтвея поддержки скриптового языка программирования и/или написания специального модуля для таких манипуляций. Аналогично, автоматическое построение мониторинговых панелей в популярных сервисах типа связки Prometheus+Grafana (да и в целом любой инструмент разбора логов) гораздо проще организовать по path, чем вычленять из данных запроса какой-то синтетический ключ группировки запросов.

    Всё это приводит нас к соображению, что поддержание одинаковой структуры URL, в которой меняется только путь или домен, а параметры всегда находятся в query и именуются одинаково, приводит к ещё более унифицированному интерфейсу, хотя бы и в ущерб читабельности и семантичности URL. Во многих внутренних системах выбор в пользу удобства выглядит самоочевидным, хотя во внешних API мы бы такой подход не рекомендовали.

#### CRUD-операции

Одно из самых популярных приложений HTTP API — это реализация CRUD-интерфейсов. Акроним CRUD (**C**reate, **R**ead, **U**pdate, **D**elete) был популяризирован ещё в 1983 году Джеймсом Мартином, но с развитием HTTP API обрёл второе дыхание. Ключевая идея соответствия CRUD и HTTP заключается в том, что каждой из CRUD-операций соответствует один из глаголов HTTP:

- операции создания — создание ресурса через метод `POST`;
- операции чтения — возврат представления ресурса через метод `GET`;
- операции редактирования — перезапись ресурса через метод `PUT` или редактирование через `PATCH`;
- операции удаления — удаление ресурса через метод `DELETE`.

**NB**: фактически, подобное соответствие — это просто мнемоническое правило, позволяющее определить, какой глагол следует использовать к какой операции. Мы, однако, должны предостеречь читателя: глагол следует выбирать по его семантике согласно стандарту, а не по мнемоническим правилам. Может показаться, что, например, операцию удаления 3-го элемента списка нужно реализовать через `DELETE`:

- `DELETE /v1/list/{list_id}/?position=3`

    но, как мы помним, делать так категорически нельзя: во-первых, такой вызов неидемпотентен; во-вторых, нарушает требование консистентности `GET` и `DELETE`.

С точки зрения удобства разработки концепция соответствия CRUD и HTTP выглядит очень удобной — каждому виду ресурсов соответствует свой URL, каждой операции — свой глагол. При пристальном рассмотрении, однако, оказывается, что это отношение — очень упрощённое представление о манипуляции ресурсами, и, что самое неприятное, плохо расширяемое.

##### [1\. Создание](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-para-1)

Начнём с операции создания ресурса. Как мы помним из главы «[Стратегии синхронизации](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-sync-strategies)», операция создания в любой сколько-нибудь ответственной предметной области обязана быть идемпотентной и, очень желательно, ещё и позволять управлять параллелизмом. В рамках парадигмы HTTP API идемпотентное создание можно организовать одним из трёх способов:

1.  Через метод `POST` с передачей токена идемпотентности (им может выступать, в частности, `ETag` ресурса):

    ```
    POST /v1/orders/?user_id=<user_id> HTTP/1.1
    If-Match: <ревизия>

    { … }
    ```

2.  Через метод `PUT`, предполагая, что идентификатор заказа сгенерирован клиентом (ревизия при этом всё ещё может использоваться для управления параллелизмом, но токеном идемпотентности является сам URL):

    ```
    PUT /v1/orders/{order_id} HTTP/1.1
    If-Match: <ревизия>

    { … }
    ```

3.  Через схему создания черновика методом `POST` и его подтверждения методом `PUT`:

    ```
    POST /v1/drafts HTTP/1.1

    { … }
    →
    HTTP/1.1 201 Created
    Location: /v1/drafts/{id}
    ```

    ```
    PUT /v1/drafts/{id}/commit
    If-Match: <ревизия>

    {"status": "confirmed"}
    →
    HTTP/1.1 200 OK
    Location: /v1/orders/{id}
    ```

Метод (2) в современных системах используется редко, так как вынуждает доверять правильности генерации идентификатора заказа клиентом. Если же рассматривать варианты (1) и (3), то необходимо отметить, что семантике протокола вариант (3) соответствует лучше, так как `POST`\-запросы по умолчанию считаются неидемпотентными, и их автоматический повтор в случае получения сетевого таймаута или ошибки сервера будет выглядеть для постороннего наблюдателя опасной операцией (которой запрос и правда может стать, если сервер изменит политику проверки заголовка `If-Match` на более мягкую). Повтор `PUT`\-запроса (а мы предполагаем, что таймауты и серверные ошибки на «тяжёлой» операции создания заказа намного более вероятны, чем на «лёгкой» операции создания черновика) вполне может быть автоматизирован, и не будет создавать дубликаты заказа, даже если проверка ревизии будет отключена вообще.

##### [2\. Чтение](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-para-2)

Идём дальше. Операция чтения на первый взгляд не вызывает сомнений:

- `GET /v1/orders/{id}`.

Стоит, однако, присмотреться внимательнее, и всё оказывается не так просто. Клиент как минимум должен обладать способом выяснить, какие заказы сейчас выполняются от его имени, что требует создания отдельного ресурса-поисковика:

- `GET /v1/orders/?user_id=<user_id>`.

Передача списков без ограничений по их длине — потенциально плохая идея, а значит необходимо ввести поддержку пагинации:

- `GET /v1/orders/?user_id=<user_id>&cursor=<cursor>`.

Если заказов много, наверняка пользователю понадобятся фильтры, скажем, по названию напитка:

- `GET /v1/orders/?user_id=<user_id>&recipe=lungo`.

Однако, если пользователь захочет видеть в одном списке и латте и лунго, этот интерфейс уже окажется ограниченно применимым, поскольку общепринятого стандарта передачи в URL более сложных структур, чем пары ключ-значение, не существует. Довольно скоро мы придём к тому, что, наряду с доступом по идентификатору заказа потребуется ещё и поисковый эндпойнт со сложной семантикой (которую гораздо удобнее было бы разместить за `POST`):

- `POST /v1/orders/search { /* parameters */ }`

Кроме того, если к заказу можно прикладывать какие-то медиа-данные (скажем, фотографии), то для доступа к ним придётся разработать отдельные URL:

- `GET /v1/orders/{order_id}/attachements/{id}`

##### [3\. Редактирование](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-para-3)

Проблемы частичного обновления ресурсов мы подробно разбирали в [соответствующей главе](https://twirl.github.io/The-API-Book/API.ru.html#api-patterns-partial-updates) раздела «Паттерны дизайна API». Напомним, что полная перезапись ресурса методом `PUT` возможна, но быстро разбивается о необходимость работать с вычисляемыми и неизменяемыми полями, необходимость совместного редактирования и/или большой объём передаваемых данных. Работа через метод `PATCH` возможна, но, так как этот метод по умолчанию считается неидемпотентным (и часто зависящим от порядка выполнения операций), для него справедливо всё то же соображение об опасности автоматических перезапросов. Достаточно быстро мы придём к одному из двух вариантов:

- либо `PUT` декомпозирован на множество составных `PUT /v1/orders/{id}/address`, `PUT /v1/orders/{id}/volume` и т.д. — по ресурсу для каждой частной операции;
- либо существует отдельный ресурс, принимающий список изменений, причём, вероятнее всего, через схему черновик-подтверждение в виде пары методов `POST` + `PUT`.

Если к сущности прилагаются медиаданные, для их редактирования также придётся разработать отдельные эндпойнты.

##### [4\. Удаление](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-para-4)

С удалением ситуация проще всего: никакие данные в современных сервисах не удаляются моментально, а лишь архивируются или помечаются удалёнными. Таким образом, вместо `DELETE /v1/orders/{id}` необходимо разработать эндпойнт типа `PUT /v1/orders/{id}/archive` или `PUT /v1/archive?order=<order_id>`.

#### CRUD-операции в реальной жизни

Изложенные выше соображения не следует считать критикой концепции CRUD как таковой. Мы лишь указываем, что в сложных предметных областях «срезание углов» и следование мнемоническим правилам редко работает. Мы начали с двух URL и четырёх-пяти методов манипуляции ими:

- `/v1/orders/` для вызова с глаголом `POST`;
- `/v1/orders/{id}` для вызова с глаголами `GET` / `PUT` / `DELETE` / опционально `PATCH`.

Однако, если мы выдвинем требования наличия возможностей:

- контроля параллелизма при создании сущностей;
- совместного редактирования;
- архивирования;
- поиска с фильтрацией;

то мы быстро придём к номенклатуре из 8 URL и 9-10 методов:

- `GET /v1/orders/?user_id=<user_id>` для получения списка текущих заказов, возможно с какими-то простыми фильтрами;
- `/v1/orders/drafts/?user_id=<user_id>` для создания черновика заказа через метод `POST` и получения списка текущих черновиков и актуальной ревизии через метод `GET`;
- `PUT /v1/orders/drafts/{id}/commit` для подтверждения черновиков и создания заказов;
- `GET /v1/orders/{id}` для получения вновь созданного заказа;
- `POST /v1/orders/{id}/drafts` для создания черновика внесения списка изменений;
- `PUT /v1/orders/{id}/drafts/{id}/commit` для подтверждения черновика со списком изменений;
- `/v1/orders/search?user_id=<user_id>` для поиска заказов через метод `GET` (простые случаи) или `POST` (если необходимы сложные фильтры);
- `PUT /v1/orders/{id}/archive` для архивирования заказа.

плюс, вероятно, потребуются частные операции типа `POST /v1/orders/{id}/cancel` для проведения атомарных изменений. Именно это произойдёт в реальной жизни: идея CRUD как методологии описания типичных операций над ресурсом с помощью небольшого набора HTTP-глаголов быстро превратится в семейство эндпойнтов, каждый из которых покрывает какой-то аспект жизненного цикла сущности. Это всего лишь означает, что CRUD-мнемоника даёт только стартовый набор гипотез; любая конкретная предметная область требует вдумчивого подхода и дизайна подходящего API. Если же перед вами стоит задача разработать «универсальный» интерфейс, который подходит для работы с любыми сущностями, лучше сразу начинайте с номенклатуры в 10 методов, подобной описанной выше.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-ref-1-back) The HTTP QUERY Method

    [www.ietf.org/archive/id/draft-ietf-httpbis-safe-method-w-body-02.html](https://www.ietf.org/archive/id/draft-ietf-httpbis-safe-method-w-body-02.html)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-urls-crud-ref-2-back) Web Distributed Authoring and Versioning (WebDAV) SEARCH

    [www.rfc-editor.org/rfc/rfc5323](https://www.rfc-editor.org/rfc/rfc5323)

### [Глава 38. Работа с ошибками в HTTP API](https://twirl.github.io/The-API-Book/API.ru.html#http-api-errors)

Рассмотренные в предыдущих главах примеры организации API согласно стандарту HTTP и принципам REST покрывают т.н. «happy path», т.е. стандартный процесс работы с API в отсутствие ошибок. Конечно, нам не менее интересен и обратный кейс — каким образом HTTP API следует работать с ошибками, и чем стандарт и архитектурные принципы могут нам в этом помочь. Пусть какой-то агент в системе (неважно, клиент или гейтвей) пытается создать новый заказ:

```
POST /v1/orders?user_id=<user_id> HTTP/1.1
Authorization: Bearer <token>
If-Match: <ревизия>

{ /* параметры заказа */ }
```

Какие потенциальные неприятности могут ожидать нас при выполнении этого запроса? Навскидку, это:

1.  Запрос не может быть прочитан (недопустимые символы, нарушение синтаксиса).
2.  Токен авторизации отсутствует.
3.  Токен авторизации невалиден.
4.  Токен валиден, но пользователь не обладает правами создавать новый заказ.
5.  Пользователь удалён или деактивирован.
6.  Идентификатор пользователя неверен (не существует).
7.  Ревизия не передана.
8.  Ревизия не совпадает с последней актуальной.
9.  В теле запроса отсутствуют обязательные поля.
10. Какое-то из полей запроса имеет недопустимое значение.
11. Превышены лимиты на допустимое количество запросов.
12. Сервер перегружен и не может ответить в настоящий момент.
13. Неизвестная серверная ошибка (т.е. сервер сломан настолько, что диагностика ошибки невозможна).

Исходя из общих соображений, соблазнительной кажется идея назначить каждой из ошибок свой статус-код. Скажем, для ошибки (4) напрашивается код `403`, а для ошибки (11) — `429`. Не будем, однако, торопиться, и прежде зададим себе вопрос _с какой целью_ мы хотим назначить тот или иной код ошибки.

В нашей системе в общем случае присутствуют три агента: пользователь приложения, само приложение (клиент) и сервер. Каждому из этих акторов необходимо понимать ответ на четыре вопроса относительно ошибки (причём для каждого из акторов ответ может быть разным):

1.  Кто допустил ошибку (конечный пользователь, разработчик клиента, разработчик сервера или какой-то промежуточный агент, например, программист сетевого стека).
    - Не забудем учесть тот факт, что и конечный пользователь, и разработчик клиента могут допустить ошибку _намеренно_, например, пытаясь перебором подобрать пароль к чужому аккаунту.
2.  Можно ли исправить ошибку, просто повторив запрос.
    - Если да, то через какое время.
3.  Если повтором запроса ошибку исправить нельзя, то можно ли её исправить, переформулировав запрос.
4.  Если ошибку вообще нельзя исправить, то что с этим делать.

На один из этих вопрос в рамках стандарта HTTP ответить достаточно легко: регулировать желаемое время повтора запроса можно через параметры кэширования ответа и заголовок `Retry-After`. Также HTTP частично помогает с первым вопросом: для определения, на чьей стороне произошла ошибка, используется первая цифра статус-кода (см. ниже).

Со всеми остальными вопросами, увы, ситуация сильно сложнее.

#### Клиентские ошибки

Статус-коды, начинающиеся с цифры `4`, индицируют, что ошибка допущена пользователем или клиентом (или, по крайней мере, сервер так считает). _Обычно_, полученную `4xx` повторять бессмысленно — если не предпринять дополнительных действий по изменению состояния сервиса, этот запрос не будет выполнен успешно никогда. Однако из этого правила есть исключения, самые важные из которых — `429 Too Many Requests` и `404 Not Found`. Последняя по стандарту имеет смысл «состояния неопределённости»: сервер имеет право использовать её, если не желает раскрывать причины ошибки. После получения ошибки `404`, можно сделать повторный запрос, и он вполне может отработать успешно. Для индикации _персистентной_ ошибки «ресурс не найден» используется отдельный статус `410 Gone`.

Более интересный вопрос — а что всё-таки клиент может (или должен) сделать, получив такую ошибку. Как мы указывали в главе «[Разграничение областей ответственности](https://twirl.github.io/The-API-Book/API.ru.html#api-design-isolating-responsibility)», если ошибка может быть исправлена программно, необходимо в машиночитаемом виде индицировать это клиенту; если ошибка не может быть исправлена, необходимо включить человекочитаемые сообщения для пользователя (даже просто «попробуйте начать сначала / перезагрузить приложение» лучше с точки зрения UX, чем «неизвестная ошибка») и для разработчика, который будет разбираться с проблемой.

С восстановимыми ошибками в HTTP, к сожалению, ситуация достаточно сложная. С одной стороны, протокол включает в себя множество специальных кодов, которые индицируют проблемы с использованием самого протокола — такие как `405 Method Not Allowed` (данный глагол неприменим к указанному ресурсу), `406 Not Acceptable` (сервер не может вернуть ответ согласно `Accept`\-заголовкам запроса), `411 Length Required`, `414 URI Too Long` и так далее. Код клиента может обработать данные ошибки и даже, возможно, предпринять какие-то действия по их устранению (например, добавить заголовок `Content-Length` в запрос после получения ошибки `411`), но все они очень плохо применимы к ошибкам в бизнес-логике. Например, мы можем вернуть `429 Too Many Requests` при превышении лимитов запросов, но у нас нет никакого стандартного способа указать, _какой именно_ лимит был превышен.

Частично проблему отсутствия стандартных подходов к возврату ошибок компенсируют использованием различных близких по смыслу статус-кодов для индикации разных состояний (либо и вовсе выбор произвольного кода ошибки и придания ему нового смысла в рамках конкретного API). В частности, сегодня де-факто стандартом является возврат кода `401 Unauthorized` при отсутствии заголовков авторизации или невалидном токене (получение этого кода, таким образом, является сигналом для приложения предложить пользователю залогиниться в системе), что противоречит стандарту (который требует при возврате `401` обязательно указать заголовок `WWW-Authenticate` с описанием способа аутентификации пользователя; нам неизвестны реальные API, которые выполняют это требованием).

Однако таких кодов, которые могут отражать нюансы одной и той же проблемы, в стандарте очень мало. Фактически, мы приходим к тому, что множество различных ошибок в логике приложения приходится возвращать под очень небольшим набором статус-кодов:

- `400 Bad Request` для всех ошибок валидации запроса (некоторые пуристы утверждают, что, вообще говоря, `400` соответствует нарушению формата запроса — невалидному JSON, например — а для логических ошибок следует использовать код `422 Unprocessable Content`; в постановке задачи это мало что меняет);
- `403 Forbidden` для любых проблем, связанных с авторизацией действий клиента;
- `404 Not Found` в случае, если какие-то из указанных в запросе сущностей не найдены _либо_ раскрытие причин ошибки нежелательно;
- `409 Conflict` при нарушении целостности данных;
- `410 Gone` если ресурс был удалён;
- `429 Too Many Requests` при превышении лимитов.

Разработчики стандарта HTTP об этой проблеме вполне осведомлены, и отдельно отмечают, что для решения бизнес-сценариев необходимо передавать в метаданных либо теле ответа дополнительные данные для описания возникшей ситуации («the server SHOULD send a representation containing an explanation of the error situation, and whether it is a temporary or permanent condition»), что (как и введение новых специальных кодов ошибок) противоречит самой идее унифицированного машиночитаемого формата ошибок. (Отметим, что отсутствие стандартов описания ошибок в бизнес-логике — одна из основных причин, по которым мы считаем разработку REST API как его описал Филдинг в манифесте 2008 года невозможной; клиент _должен_ обладать априорным знанием о том, как работать с метаинформацией об ошибке, иначе он сможет восстанавливать своё состояние после ошибки только перезагрузкой.)

**NB**: не так давно разработчики стандарта предложили собственную версию спецификации JSON-описания HTTP-ошибок — RFC 9457<sup><a id="http-api-errors-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-errors-ref-1">1</a></sup>. Вы можете воспользоваться ей, но имейте в виду, что она покрывает только самый базовый сценарий:

- подтип ошибки не передаётся в мета-информации;
- нет разделения на сообщение для пользователя и сообщение для разработчика;
- конкретный машиночитаемый формат описания ошибок остаётся на усмотрение разработчика.

Дополнительно, у проблемы есть и третье измерение в виде серверного ПО мониторинга состояния системы, которое часто полагается на статус-коды ответов при построении графиков и уведомлений. Между тем, ошибки, скрывающиеся под одним статус кодом — например ввод неправильного пароля и истёкший срок жизни токена — могут быть очень разными по смыслу; повышенный фон первой ошибки может говорить о потенциальной попытке взлома путём перебора паролей, а второй — о потенциальных ошибках в новой версии приложения, которая может неверно кэшировать токены авторизации.

Всё это естественным образом подводит нас к следующему выводу: если мы хотим использовать ошибки для диагностики и (возможно) восстановления состояния клиента, нам необходимо добавить машиночитаемую метаинформацию о подвиде ошибки и, возможно, тело ошибки с указанием подробной информации о проблемах — например, как мы предлагали в главе «[Описание конечных интерфейсов](https://twirl.github.io/The-API-Book/API.ru.html#api-design-describing-interfaces)»:

```
POST /v1/coffee-machines/search HTTP/1.1

{
  "recipes": ["lngo"],
  "position": {"latitude": 110, "longitude": 55}
}
→
HTTP/1.1 400 Bad Request
X-OurCoffeeAPI-Error-Kind: wrong_parameter_value

{
  "reason": "wrong_parameter_value",
  "localized_message": "Что-то пошло не так.↵
     Обратитесь к разработчику приложения.",
  "details": { "checks_failed": [
    { "field": "recipe",
      "error_type": "wrong_value",
      "message": "Value 'lngo' unknown.↵
          Did you mean 'lungo'?" },
    { "field": "position.latitude",
      "error_type": "constraint_violation",
      "constraints": { "min": -90, "max": 90},
      "message": "'position.latitude' value↵
        must fall within the [-90, 90] interval" }
  ]}
}
```

Также напомним, что любые неизвестные `4xx`\-статус-коды клиент должен трактовать как ошибку `400 Bad Request`, следовательно, формат (мета)данных ошибки `400` должен быть максимально общим.

#### Серверные ошибки

**Ошибки `5xx`** индицируют, что клиент, со своей стороны, выполнил запрос правильно, и проблема заключается в сервере. Для клиента, по большому счёту, важно только то, имеет ли смысл повторять запрос и, если да, то через какое время. Если учесть, что в любых публично доступных API причины серверных ошибок, как правило, не раскрывают — в абсолютном большинстве кодов `500 Internal Server Error` и `503 Service Unavailable` достаточно для индикации серверных ошибок (второй код указывает, что отказ в обслуживании имеет разовый характер и есть смысл автоматически повторить запрос), или можно вовсе ограничиться одним из них с опциональным заголовком `Retry-After`.

Для внутренних систем, вообще говоря, такое рассуждение неверно. Для построения правильных мониторингов и системы оповещений необходимо, чтобы серверные ошибки, точно так же, как и клиентские, содержали подтип ошибки в машиночитаемом виде. Здесь по-прежнему применимы те же подходы — использование широкой номенклатуры кодов и/или передача типа ошибки заголовком — однако эта информация должна быть вырезана гейтвеем на границе внешней и внутренней систем, и заменена на общую информацию для разработчика и для конечного пользователя системы с описанием действий, которые необходимо выполнить при получении ошибки.

```
POST /v1/orders/?user_id=<user id> HTTP/1.1
If-Match: <ревизия>

{ "parameters" }
→
// Ответ, полученный гейтвеем
// от сервиса обработки заказов,
// метаданные которого будут
// использованы для мониторинга
HTTP/1.1 500 Internal Server Error
// Тип ошибки: получен таймаут от БД
X-OurCoffeAPI-Error-Kind: db_timeout
{ /*
   * Дополнительные данные, например,
   * какой хост ответил таймаутом
   */ }
```

```
// Ответ, передаваемый клиенту.
// Детали серверной ошибки удалены
// и заменены на инструкцию клиенту.
// Поскольку гейтвей не знает, был
// ли в действительности сделан заказ,
// клиенту рекомендуется попробовать
// повторить запрос и/или попытаться
// получить актуальное состояние
HTTP/1.1 500 Internal Server Error
Retry-After: 5

{
  "reason": "internal_server_error",
  "localized_message": "Не удалось↵
    получить ответ от сервера.↵
    Попробуйте повторить операцию
    или обновить страницу.",
  "details": {
    "can_be_retried": true,
    "is_operation_failed": "unknown"
  }
}
```

Вот здесь мы, однако, вступаем на очень скользкую территорию. Современная практика реализации HTTP-клиентов такова, что безусловно повторяются только немодифицирующие (`GET`, `HEAD`, `OPTIONS`) запросы. В случае модифицирующих запросов _разработчик должен написать код_, который повторит запрос — и для этого разработчику нужно очень внимательно прочитать документацию к API, чтобы убедиться, что это поведение допустимо и не приведёт к побочным эффектам.

_Теоретически_ идемпотентные методы `PUT` и `DELETE` можно вызывать повторно. Практически, однако, ввиду того, что многие разработчики упускают требование идемпотентности этих методов, фреймворки работы с HTTP API по умолчанию перезапросов модифицирующих методов, как правило, не делают, но некоторую выгоду из следования стандарту мы всё же можем извлечь — по крайней мере, сама сигнатура индицирует, что запрос _можно_ повторять.

Что касается более сложных ситуаций, когда мы хотим указать разработчику, что он может безопасно повторить потенциально неидемпотентную операцию, то мы могли бы предложить формат описания доступных действий в теле ошибки… но практически никто не ожидает найти такое описание в самой ошибке. Возможно, потому, что с ошибками `5xx`, в отличие от `4xx`, программисты практически не сталкиваются при написании клиентского кода, и мало какие тестовые среды позволяют такие ошибки эмулировать. Так или иначе, описывать необходимые действия при получении серверной ошибки вам придётся в документации. (Имейте в виду, что эти инструкции с большой долей вероятности будут проигнорированы. Таков путь.)

#### Организация системы ошибок в HTTP API на практике

Как понятно из вышесказанного, фактически есть три способа работать с ошибками HTTP API:

1.  Расширительно трактовать номенклатуру статус-кодов и использовать новый код каждый раз, когда требуется индицировать новый вид ошибки. (Автор этой книги неоднократно встречал ситуации, когда при разработке API просто выбирался «похоже выглядящий» статус безо всякой оглядки на его описание в стандарте.)
2.  Полностью отказаться от использования статус-кодов и вкладывать описание ошибки в тело и/или метаданные ответа с кодом `200`. Этим путём идут почти все RPC-фреймворки.
    - 2а. Вариантом этой стратегии можно считать использование всего двух статус-кодов ошибок (`400` для любой клиентской ошибки, `500` для любой серверной), опционально трёх (те же плюс `404` для статуса неопределённости).

3.  Применить смешанный подход, то есть использовать статус-код согласно его семантике для индикации _рода_ ошибки и вложенные (мета)данные в специально разработанном формате для детализации (подобно фрагментам кода, предложенным нами в настоящей главе).

Как нетрудно заметить, считать соответствующим стандарту можно только подход (3). Будем честны и скажем, что выгоды следования ему, особенно по сравнению с вариантом (2а), не очень велики и состоят в основном в чуть лучшей читабельности логов и большей прозрачности для промежуточных прокси.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-errors-ref-1-back) RFC 9457 Problem Details for HTTP APIs

    [www.rfc-editor.org/rfc/rfc9457.html](https://www.rfc-editor.org/rfc/rfc9457.html)

### [Глава 39. Заключительные положения и общие рекомендации](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations)

Подведём итог описанному в предыдущих главах. Чтобы разработать качественный HTTP API, необходимо:

1.  Описать happy path, т.е. составить диаграмму вызовов для стандартного цикла работы клиентского приложения.
2.  Определить каждый вызов как операцию над некоторым ресурсом и, таким образом, составить номенклатуру URL и применимых методов.
3.  Понять, какие ошибки возможны при выполнении операций и каким образом клиент должен восстанавливаться из какого состояния.
4.  Решить, какая функциональность будет передана на уровень протокола HTTP \[какие стандартные возможности протокола будут использованы в сопряжении с какими инструментами разработки\] и в каком объёме.
5.  Опираясь на решения 1-4, разработать конкретную спецификацию.
6.  Проверить себя: пройти по пунктам 1-3, написать псевдокод бизнес-логики приложения согласно разработанной спецификации, и оценить, насколько удобным, понятным и читабельным оказался результирующий API.

Позволим себе так же дать несколько советов по code style:

1.  Не различайте пути с `/` на конце и без него и примите какую-то рекомендацию по умолчанию (мы рекомендуем все пути заканчивать на `/` — по простой причине, это позволяет разумно описать обращение к корню домена как `ГЛАГОЛ /`). Если вы решили запретить один из вариантов (скажем, пути без слэша в конце), при обращении по второму варианту должен быть или редирект или однозначно читаемая ошибка.
2.  Включайте в ответы стандартные заголовки — `Date`, `Content-Type`, `Content-Encoding`, `Content-Length`, `Cache-Control`, `Retry-After` — и вообще старайтесь не полагаться на то, что клиент правильно догадывается о параметрах протокола по умолчанию.
3.  Поддержите метод `OPTIONS` и протокол CORS<sup><a id="http-api-final-recommendations-ref-1-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-1">1</a></sup> на случай, если ваш API захотят использовать из браузеров.
4.  Определитесь с правилами выбора кейсинга параметров (и преобразований кейсинга при перемещении параметра между различными частями запроса) и придерживайтесь их.
5.  Всегда оставляйте себе возможность обратно-совместимого расширения операции API. В частности, всегда возвращайте корневой JSON-объект в ответах эндпойтов — потому что приписать новые поля к объекту вы можете, а к массивам и примитивам — нет.
    - Отметим также, что пустая строка не является валидным JSON, поэтому корректнее возвращать пустой объект `{}` там, где ответа не подразумевается (или статус `204 No Content` с пустым телом, но тогда эндпойнт нельзя будет расширить в будущем).

6.  Для всех `GET`\-запросов указывайте политику кэширования (иначе всегда есть шанс, что клиент или промежуточный агент придумает её за вас).
7.  Не эксплуатируйте известные возможности оперировать запросами в нарушение стандарта и не изобретайте свои решения для «серых зон» протокола. В частности:
    - не размещайте модифицирующие операции за методом `GET` и неидемпотентные операции за `PUT` / `DELETE`;
    - соблюдайте симметрию `GET` / `PUT` / `DELETE` методов;
    - не позволяйте `GET` / `HEAD` / `DELETE`\-запросам иметь тело, не возвращайте тело в ответе метода `HEAD` или совместно со статус-кодом `204 No Content`;
    - не придумывайте свой стандарт для передачи массивов и вложенных объектов в query — лучше воспользоваться HTTP-глаголом, позволяющим запросу иметь тело, или, в крайнем случае, передать параметры в виде Base64-кодированного JSON-поля;
    - не размещайте в пути и домене URL параметры, по формату требующие эскейпинга (т.е. могущие содержать символы, отличные от цифр и букв латинского алфавита); для этой цели лучше воспользоваться query-параметрами или телом запроса.

8.  Ознакомьтесь хотя бы с основными видами уязвимостей в типичных имплементациях HTTP API, которыми могут воспользоваться злоумышленники:
    - CSRF<sup><a id="http-api-final-recommendations-ref-2-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-2">2</a></sup>
    - SSRF<sup><a id="http-api-final-recommendations-ref-3-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-3">3</a></sup>
    - HTTP Response Splitting<sup><a id="http-api-final-recommendations-ref-4-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-4">4</a></sup>
    - Unvalidated Redirects and Forwards<sup><a id="http-api-final-recommendations-ref-5-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-5">5</a></sup>

    и заложите защиту от этих векторов атак на уровне вашего серверного ПО. Организация OWASP предоставляет хороший обзор лучших security-практик для HTTP API<sup><a id="http-api-final-recommendations-ref-6-back" href="https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-6">6</a></sup>.

В заключение хотелось бы сказать следующее: HTTP API — это способ организовать ваше API так, чтобы полагаться на понимание семантики операций как разнообразным программным обеспечением, от клиентских фреймворков до серверных гейтвеев, так и разработчиком, который читает спецификацию. В этом смысле экосистема HTTP предоставляет, пожалуй, наиболее широкий (и в плане глубины, и в плане распространённости) по сравнению с другими технологиями словарь для описания самых разнообразных ситуаций, возникающих во время работы клиент-серверных приложений. Разумеется, эта технология не лишена своих недостатков, но для разработчика _публичного_ API она является выбором по умолчанию — на сегодняшний день скорее надо обосновывать отказ от HTTP API чем выбор в его пользу.

#### Примечания

- [<sup>1</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-1-back) Fetch Living Standard. CORS protocol

    [fetch.spec.whatwg.org/#http-cors-protocol](https://fetch.spec.whatwg.org/#http-cors-protocol)

- [<sup>2</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-2-back) Cross Site Request Forgery (CSRF)

    [owasp.org/www-community/attacks/csrf](https://owasp.org/www-community/attacks/csrf)

- [<sup>3</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-3-back) Server Side Request Forgery

    [owasp.org/www-community/attacks/Server_Side_Request_Forgery](https://owasp.org/www-community/attacks/Server_Side_Request_Forgery)

- [<sup>4</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-4-back) HTTP Response Splitting

    [owasp.org/www-community/attacks/HTTP_Response_Splitting](https://owasp.org/www-community/attacks/HTTP_Response_Splitting)

- [<sup>5</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-5-back) Unvalidated Redirects and Forwards Cheat Sheet

    [cheatsheetseries.owasp.org/cheatsheets/Unvalidated_Redirects_and_Forwards_Cheat_Sheet.html](https://cheatsheetseries.owasp.org/cheatsheets/Unvalidated_Redirects_and_Forwards_Cheat_Sheet.html)

- [<sup>6</sup>](https://twirl.github.io/The-API-Book/API.ru.html#http-api-final-recommendations-ref-6-back) REST Security Cheat Sheet

    [cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html](https://cheatsheetseries.owasp.org/cheatsheets/REST_Security_Cheat_Sheet.html)
